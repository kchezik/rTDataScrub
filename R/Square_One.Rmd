---
title: "Square_One"
author: "Kyle Chezik"
date: "5/18/2018"
output: html_document
---

```{r StartUp Stan}
#Get necessary libraries.
library(tidyverse); library(rstan); library(lubridate)
#Initiate cores and make sure unchanged models are not recompiled.
options(mc.cores = parallel::detectCores(), auto_write = TRUE)
```

## Data Simulation and Parameter Estimation Functions

These functions create simulated data and offer tools to estimate parameter priors.

```{r Generative Functions}
### Data Generating Functions ###

# Logistic curve relating air and water temperature (i.e., Global Model).
alpha_est = function(){
  #k = rnorm(1,.15,.01) #Rate of change between max and min mean t.
  k = .15
  #b = rnorm(1,0,.01) #Minimum mean temperature.
  b = 0
  #alpha_max = rnorm(1,27,.1)-b #Maximum mean temperature minus the minimum mean temperature.
  alpha_max = 27
  #m = rnorm(1,16,.1) #Air temperature that is the mid point between alpha_max and b.
  m = 16
  data.frame(alpha_max, k, b, m)
}
# Create annual temperature curves.
create = function(A, AR, s_t, meanT, years = 1, tao, df = 3, snow=NA, source = "air"){
  # Number of samples.
  N = years*365 * 24
  # Generic season cycle.
  d_cycle = cos((years*2)*pi*1:N/N + tao*pi)
  #Variance by season.
  sd = 1/exp(abs(diff(d_cycle))); sd = (scale(sd)+2)/s_t; sd = c(sd[,1], sd[1,1])
  # Daily cycle by hour.
  h_cycle = sd*(cos((years*2)*pi*1:N/24 + tao*pi))
  # Seasonally adjusted AR1 correlated noise.
  noise = arima.sim(model = list(ar = AR), n = N, 
                    rand.gen = function(n, ...) rt(n, df = df))
  #Possible snow effect for water.
  if(source == "water"){
    #Determine first derivative of each data point.
    z = -sin((years*2)*pi*1:N/N + tao*pi)
    #Determine the day in the spring associated with positive slopes
    #Return 0 for days outside of spring and early summer
    day = if_else(z>0,((asin(-1*z)-pi)*N)/(2*pi),0)
    #Calcuate snow effect for each day in the spring and early summer.
    adj = if_else(day!=0,sin(2*pi*day/N+pi),0)
    snow_adj = adj * snow
  }
  #Return combined data.
  if(source == "air") dat = (d_cycle * A + meanT) + h_cycle + noise*sd
  if(source == "water") dat = (d_cycle * A + meanT + snow_adj) + h_cycle + noise*sd
  
  dat
}
# Extract chunks of data randomly within a year.
period = function(df, col){
  library(tidyverse);library(lubridate)
  init = sample_n(df, 1)$time
  fin = init  + ddays(round(psych::logistic(rnorm(n = 1, mean = psych::logit(yday(init)/365), sd = 1))*365))
  names(df)[[col]] = "temperature"
  filter(df, time >= init, time <= fin) %>% select(time, temperature)
}
# Eliminate data from the dataframe that has been used so there isn't overlap.
reduce = function(df, sub_df){
  df[!(df$time %in% sub_df$time),]
}
# This function is a wrapper that uses create(), period() and reduce() to generate random data over the course of a year that is made up of air and water sources.
generate = function(air_ends = F, air = F, years = 1, randomize = c(T,T), global){
  library(tidyverse); library(lubridate)
  
  # Sample location latitude.
  latitude = rnorm(1,50,4) #45 to 60ºN with a mean of 50.
  
  #Air parameters. http://www-das.uwyo.edu/~geerts/cwx/notes/chap16/geo_clim.html
  air_A = rnorm(1, .45*latitude, latitude*.02)/2 # Temperature range is latitude dependant.
  air_AR = rbeta(n = 1, shape1 = 500, shape2 = 25)
  air_meanT = 27-(latitude-16)*.57 + rnorm(1,0,.25)
 #.65 
  #Water parameters.
  water_meanT = rnorm(1,(global$alpha_max/(1+exp(-global$k*(air_meanT-global$m))))+global$b,0.1)
  
  #relationship between W_mean and W_A.
  alpha_inf = 13.5
  if(water_meanT<alpha_inf) water_A = water_meanT + rnorm(1, 0, .1)
  else water_A = (alpha_inf - (water_meanT-alpha_inf)) + rnorm(1, 0, .1)
  water_A = if_else(water_A>air_A, air_A-0.01, water_A)
  
  water_AR = rbeta(n = 1, shape1 = 400, shape2 = 20)
  
  #Strength of snow effect.
  snow = rbeta(1,1,1)*3
  
  #Seasonal Period Parameter
  tao = rbeta(1,1,1)*2+0.035 # Randomize sampling period.
  
  #Create Data.
  df = data.frame(
    air= create(A=air_A, AR=air_AR, s_t=15, meanT=air_meanT, years=years, tao=tao+0.035, df = 7),
    water= create(A=water_A, AR=water_AR, s_t=20, meanT=water_meanT, years=years, tao=tao-0.035, df=10,
                  snow = snow, source = "water"),
  
    # Add a time component so the data can be sampled.
    time = seq(from = ymd_h("2000-07-15 00") + ddays(round(182.5*tao)),
               by = "hour", length.out = years*365*24))
  
  #Add air to the begining and end.
  if(randomize[1] == T & air_ends == F) {
    ans = sample(x = c(0,1), size = 1, prob = c(.1,.9))
    if(ans == 1) air_ends = T
    ans = 0
  }
  if(air_ends == T){
    ends = filter(df,time <= floor_date(min(time), unit = "day")+dhours(23) | time >= floor_date(max(time))-dhours(23)) %>% 
      select(time, air) %>% 
      rename(., temperature = air) %>% 
      mutate(source = "air")
    df = reduce(df, ends)
  }
  
  #Select air error region.
  if(randomize[2] == T & air == F) {
    ans = sample(x = c(0,1), size = 1, prob = c(.3,.7))
    if(ans == 1) air = T
    ans = 0
  }
  if(air == T){
    airT = period(df, 1) %>%
      mutate(source = "air")
    df = reduce(df, airT)
  }
  
  #Fill remaining time period with water data.
  water = df %>% select(time, water) %>% rename(., temperature = water) %>% 
    mutate(source = "water")
  
  #Combine and return a mixed dataset.
  fin = water
  if(air_ends == T) fin = suppressWarnings(bind_rows(ends, fin))
  if(air == T) fin = suppressWarnings(bind_rows(fin, airT))
  fin = arrange(fin, time)
  
  cfs = data.frame(latitude, air_tao = tao+0.035, water_tao = tao-0.035,
                   alpha_max = global$alpha_max, 
                   k = global$k, b = global$b, 
                   aw_infl = global$m,
                   air_AR,water_AR,
                   air_A, water_A,
                   air_meanT, water_meanT, snow)
  list(dat = fin, coefs = cfs)
}

# This function is a wrapper that creates a dataset made up of many simulated sites and years of air/water data using the generate function.
multi_site = function(n = 1, years = 1, air_ends = F, air = F, randomize = c(T,T)){
  #Create list to be filled.
  ret = list(dat = NULL, coefs = NULL)
  #Generate hourly data `n` number of sites.
  for(i in 1:n){
    #Gather global model coefficients.
    global = alpha_est()
    #Generate hourly data made from air and water sources.
    df = generate(air = air, air_ends = air_ends, 
                  randomize = randomize, years = years, global = global) 
    #Add site number and period in annual cycle.
    df$dat = mutate(df$dat, site = i, 
                    d_cycle = (yday(time)*24)-(24-hour(time)),
                    n = n_init(time))
    df$coefs = mutate(df$coefs, site = i)
    #Bind data and coefs into list.
    ret$dat = suppressWarnings(bind_rows(ret$dat, df$dat))
    ret$coefs = suppressWarnings(bind_rows(ret$coefs, df$coefs))
  }
  #Rename lists
  dat = list("hourly" = ret$dat, "coefs" = ret$coefs)
  #Daily summary
  ret = ret$dat %>% mutate(time = lubridate::floor_date(time, unit = "day")) %>% 
    group_by(site, source, time) %>%
    summarize(temperature = mean(temperature)) %>% 
    arrange(site, time)
  #Remove duplicate days due to rounding errors.
  dups = ret %>% group_by(site,time) %>% summarise(c = n()) %>% filter(c>1)
  del = vector(mode = "numeric",length = length(dups)/2); c=1
  if(nrow(dups)>0){
    for (i in 1:nrow(dups)){
      rows = which(ret$time==dups[i,"time"][[1]] & ret$site == dups[i,"site"][[1]])
      del[c] = sample(rows,1)
      c=c+1
    }
  dat$daily = ret[-del,]
  } else dat$daily = ret
  
  #Add cycle length (`n`) and point in cycle (`d`).
  year_ref = data.frame(year = unique(year(dat$daily$time)), 
                        year_no = c(1:length(unique(year(dat$daily$time)))))
  dat$daily = mutate(dat$daily,
                     year = year(time),
                     d = yday(time),
                     n = n_init(time)) %>% 
    left_join(., year_ref) %>% group_by(site, year) %>% 
    mutate(reset = if_else(time==min(time),1,if_else(time==max(time),2,0)))
  
  dat
}


### Functions that approximate values to constraine the search space. ###

#Provide the number of observations in a year. Will determine the number of periods.
n_init = function(time){
  library(lubridate)
  if(length(time) == 1) cc = 365
  else {
    dur = data.frame(table(as.numeric(as.duration(time[2:length(time)]-time[1:(length(time)-1)])))) %>% 
      arrange(desc(Freq)) %>% as.tbl(.)
    dur = as.numeric(as.character(dur[[1]]))[1]
    cc = as.numeric(dyears(1)/dur[1])
  }
  if(leap_year(year(time[1]))) cc = cc + 1
  else cc = cc
  cc
}

#Build initial conditions for different numbers of chains.
create_inits = function(K = 2, A_ij = c(.9,.1,.1,.9), air_A, air_mean, chains = 1, sites = 1){
  #Create single chain list
  A_ij = diag(K); A_ij = if_else(A_ij ==1, .9, .1/(K-1))
  init = list(A_ij = matrix(A_ij, nrow = K, ncol = K),
              A = matrix(data = c(air_mean,air_A), nrow=length(air_A) , ncol=2),
              alpha_w = if_else(air_mean>0,air_mean,2),
              tau_est = matrix(c(.9,1.1,1.5),nrow = sites, ncol = 3,byrow = T))
  init$A[,1] = if_else(init$alpha_w<13.5, init$alpha_w, (27-init$alpha_w))
  init$A[,1] = if_else(init$A[,1]>=init$A[,2], init$A[,2]-.5, init$A[,1])
  
  if(length(air_A) == 1){
    init$A = init$A[1,]
    init$tau_est = init$tau_est[1,]
  }
    
  #Replicate single list over n chains
  inits = list()
  for(i in 1:chains){
    inits[[i]] = init
  }
  #return inits
  inits
}

#Geometric mean
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
}
```

## Plot Model Results

These functions offer plotting capabilities after running a Hidden Markov Model.

```{r Plot Probabilities}
prob_plot = function(mod, date, temperature, air = NULL, smooth = T, err_perc = NULL,
                     bc_climate = list(air_mean = NA, air_A = NA),dur = "daily"){
  
  #Build dataframe
  if(is.null(err_perc)) df = as.tibble(data.frame(date,temperature)) %>% arrange(date)
  else df = as.tibble(data.frame(date,temperature,source = err_perc)) %>% arrange(date) 
  #Add d and n values.
  if(dur == "hour") {
    df = df %>% mutate(year = lubridate::year(date), d = (yday(date)*24)+(hour(date)-24)+1) %>% 
      group_by(year) %>% mutate(n = n_init(date)*2)
  }
  else {
    df = df %>% mutate(year = lubridate::year(date), d = yday(date)) %>%
      group_by(year) %>% mutate(n = n_init(date))
  }
  
  #Coefficients
  cf = rstan::extract(mod, pars = c("alpha_w","A","tau_est","snow"))
  #Produce full bayesian probabilities
  fit_w = data.frame(t(sapply(c(1:nrow(df)), function(x){
    quantile(cf$alpha_w + cf$A[,1] * cos(2*pi*df[x,"d"][[1]]/df[x,"n"][[1]] + cf$tau_est[,1]*pi) +
               cf$snow*cos(2*pi*df[x,"d"][[1]]/(df[x,"n"][[1]]/2) + cf$tau_est[,3]*pi),
             probs = c(0.025,.25,0.5,.75,.975))
  }))) %>% 
    rename(w_lower = X2.5., w_lower_mid = X25., w_fit = X50., w_upper_mid = X75., w_upper = X97.5.)
  
  fit_a = data.frame(t(sapply(c(1:nrow(df)), function(x){
    quantile(bc_climate$air_mean + bc_climate$air_A * cos(2*pi*df[x,"d"][[1]]/df[x,"n"][[1]] + cf$tau_est[,2]*pi),
             probs = c(0.025,.25,0.5,.75,.975))
  }))) %>% 
    rename(a_lower = X2.5., a_lower_mid = X25., a_fit = X50., a_upper_mid = X75., a_upper = X97.5.)
  
  #Extract log-/probability estimates and their upper and lower quartiles
  if(smooth == T) xi_est = rstan::extract(mod,"gamma")$gamma
  else xi_est = rstan::extract(mod,"alpha_tk")$alpha_tk
  
  water = data.frame(t(sapply(c(1:nrow(df)),function(x){
    quantile(xi_est[,x,1],probs = c(0.025,.25,0.5,.75,.975))
  }))) %>% 
        rename(p_lower = X2.5., p_lower_mid = X25., p_prob = X50., p_upper_mid = X75., p_upper = X97.5.)
  
  #Combine fit and probabilities with data and add error columns
  df = df %>% bind_cols(.,water,fit_w,fit_a) %>% 
    mutate(est_source = if_else(p_prob>.5,"water","air"))
  if(!is.null(err_perc)){
    df = df %>% mutate(err = if_else(source == est_source, 1, 19))
    percent_error = round((1-sum(df$source==df$est_source)/nrow(df))*100)
  }
  
  #Build Plots

  #Probability plot
  p1p = ggplot(df) + 
    geom_line(aes(date, p_prob), size = .2, linetype = 3) +
    geom_ribbon(aes(date, ymin = p_lower, ymax = p_upper), alpha =.3, fill = "#5b0982") +
    scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
    ggthemes::theme_tufte() +
    theme(axis.text = element_text(size = 8),axis.title = element_text(size = 9)) +
    labs(x = "", y = "% Prob. Water")
  
  #Raw/Probability
  p1 = ggplot(df) + 
    #geom_line(aes(date, temperature), size = .2) +
    geom_point(aes(date, temperature, col = p_prob), size = .2, shape = 19) +
    geom_hline(yintercept = 0, linetype = 2, size = .2) +
    scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
    viridis::scale_color_viridis(direction = -1) +
    ggthemes::theme_tufte() +
    theme(legend.position = "top", legend.direction = "horizontal",
          axis.line.x = element_line(color="black", size = .2),
          legend.key.width = unit(1,"cm"),
          legend.key.height = unit(.20,"cm"),
          strip.text = element_text(size=8), axis.text = element_text(size = 8),
          axis.title = element_text(size = 9), legend.title = element_text(size = 8),
          legend.text = element_text(size = 8)) +
    labs(color ="Probability Water", x = "", y = expression("Temperature ("*degree*"C)")) +
    guides(colour = guide_colourbar(title.position="top"))
  
  #Best Fit
  p1f = ggplot(df, aes(date,temperature)) 
  if(!is.null(air)){
    p1f = p1f + geom_point(data = air, aes(as.POSIXct(date), mean_temp_c),
                           color = "red", shape = 1, size = .3, alpha = .2) +
      geom_line(aes(as.POSIXct(date), a_fit), size = .3) +
      geom_ribbon(aes(as.POSIXct(date), ymin = a_lower, ymax = a_upper), 
                alpha = .1, fill = "#ffc021")
  }
  if(!is.null(err_perc)){
    p1f = p1f + geom_rug(data = filter(df, err == 19), aes(date, temperature), sides = "b")+
      ggtitle(paste(percent_error,"% error", sep = ""))
  }
  
  p1f = p1f + geom_point(aes(color = est_source), size = 0.2, shape = 19) +
    geom_line(aes(date, w_fit), size = .3) +
    geom_ribbon(aes(date, ymin = w_lower, ymax = w_upper), 
                alpha = .1, fill = "#5b0982") +
    scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
    scale_color_manual(values = c("water" = "#2b6aff", "air" = "#ffc021", "ground" = "#45062e")) +
    geom_hline(yintercept = 0, size = .2, linetype = 3) +
    labs(color = "Estimated Thermal Source", x = "Month", y = expression("Temperature ("*degree*"C)")) +
    ggthemes::theme_tufte() +
    theme(legend.direction = "horizontal", legend.position = "top",
      axis.line.x = element_line(color = "black", size = .2),
      strip.text = element_text(size=8), axis.text = element_text(size = 8), 
      axis.title = element_text(size = 9), legend.title = element_text(size = 8),
      legend.text = element_text(size = 8), plot.title = element_text(size = 9)) +
    guides(colour = guide_legend(title.position="top"))
  
  p = gridExtra::grid.arrange(p1p, p1, p1f, layout_matrix = matrix(data = c(1,2,2,3,3), nrow = 5, ncol = 1))
  ggsave(filename = "probPlot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 4.25, height = 6.5, units = "in", device = "pdf")
  #list(p1p,p1,p1f)
}
```

## Seasonal Cycle Temperature Model

We are using a seasonal cycle model...
   `y[t] = alpha + A*cos(Y*2*pi*w + tau*pi) + sigma_t`, 
      `sigma_t ~ (cos(2*pi*w*t + tau)+2)*sigma`,
where `alpha` represents the vertical adjustment (i.e, mean) of the cosine curve and `A` refers to its expansion (i.e., amplitude). `Y` controls the number of cycles (i.e, years) the data contain and `w` is the frequency of the cycle and equals `t`/`N` where `t` is the current time point and `N` is the total number of time points in one cycle or in this case one year. `tau` controls where in the seasonal temperature cycle we begin observing data. The errors are seasonally adjusted so they are largest during winter and summer and least in the spring and fall. We created this model similar to that on page 67 of **Time Series Analysis and Its Applications** by Robert H. Shumway and David S. Stoffer.

Now we'll recapture `alpha`, `A`, `tau`, `rho` and `sigma`.

```{r Recapture Simulated Air & Water}
#create data
df = multi_site(n = 1, years = 1, air_ends = F, air = F, randomize = c(F,F))

#Compile and run model.
compile = stan_model("./stan/01_AW_sim.stan")
mod = sampling(compile,
               data = list(N = length(df$daily$temperature), y = df$daily$temperature),
               iter = 1000, chains = 4)

#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)
df$daily = df$daily %>% mutate(y_hat = cf$alpha + cf$A*cos(2*pi*d/365 + cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)

# Plot temperature by site.
ggplot() +
  geom_ribbon(data = df$daily, aes(time, ymin = y_hat - sd, ymax = y_hat + sd), fill = "red", alpha = 0.1) +
  geom_point(data = df$daily, aes(time, temperature), color = "red", size = .8) + 
  geom_line(data = df$daily, aes(time, temperature), size = .2) + 
  geom_line(data = df$daily, aes(time, y_hat), color = "blue") + 
  geom_hline(yintercept = 0) +
  facet_wrap(~site) + ggthemes::theme_tufte()
```

Above I demonstrate that we can easily recapture the coefficients for the general case annual temperature curve. Now we'll apply this model to some known water and air temperature data to see if they fit the data well.

```{r Real Temperature Fit}
#Real data.
dat = read_rds(path = "./Data/test_labeled.rds")
air = dat %>% filter(source == "air", !is.na(temperature)) %>% arrange(date)
water = dat %>% filter(source == "water", !is.na(temperature)) %>% arrange(date)

#air mod
mod = sampling(compile, data= list(N = length(air$temperature), y = air$temperature),
                        iter = 1000, chains = 4)

#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)

#collect results
air = air %>% mutate(d = c(1:length(temperature)),
                    y_hat = cf$alpha + cf$A*cos(2*pi*d/length(temperature)+ cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)

#water mod
mod = sampling(compile, data= list(N = length(water$temperature), y = water$temperature),
               iter = 1000, chains = 4)
               
#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)

#collect results
water = water %>% mutate(d = c(1:length(temperature)),
                    y_hat = cf$alpha + cf$A*cos(2*pi*d/length(temperature) + cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)                       

#plot results
ggplot() +
  geom_ribbon(data = air, aes(date, ymin = y_hat - sd, ymax = y_hat + sd), fill = "red", alpha = 0.1) +
  geom_point(data = air, aes(date, temperature), color = "red", size = .8) + 
  geom_ribbon(data = water, aes(date, ymin = y_hat - sd, ymax = y_hat + sd), fill = "blue", alpha = 0.4) +
  geom_point(data = water, aes(date, temperature), color = "blue", size = .8) + 
  geom_line(data = air, aes(date, y_hat), color = "black") + 
  geom_line(data = water, aes(date, y_hat), color = "black") + 
  geom_hline(yintercept = 0) +
  ggthemes::theme_tufte()
```

The model fits the real water and air temperature data well and captures the major differences between the two temperature sources. This is encouraging that the models, though the same, may have such different parameter values they will be seperable in the HMM. One noteable difference is that water temperature tends to increase slowly in the spring due to snow melt resulting in hysteresis in the temperature curve.

## HMM Model

Now we need to combine the air and water temperature data and build a Hidden Markov Model that pulls them apart.

```{r Recapture Simulated Multi-Sources w/HMM}
df = multi_site(n = 1, years = 1, air_ends = T, air = T, randomize = c(F,F))
inits = create_inits(air_A = df$coefs$air_A, air_mean = df$coefs$air_meanT, chains = 4)

ggplot(df$daily, aes(time, temperature, color = source))  +geom_point()

#Compile and run model.
compile = stan_model("./stan/02_HMM.stan")
mod = sampling(compile, data= list(N = nrow(df$daily), K = 2,
                                  y = round(df$daily$temperature,2),
                                  d = df$daily$d,
                                  n = df$daily$n,
                                  air_A = df$coefs$air_A,
                                  air_mean = df$coefs$air_meanT),
               pars = c("alpha_w","A","tau_est","snow","sigma","alpha_tk"),
               init = inits, warmup = 500, iter = 1000, chains = 4, thin = 1,
               control = list(adapt_delta = .8, max_treedepth = 10), save_warmup = T)

#Plot
prob_plot(mod, df$daily$time, df$daily$temperature, smooth = F, err_perc = df$daily$source,
          bc_climate = list(air_mean = df$coefs$water_meanT, air_A = df$coefs$air_A))
```

I did not include auto-regressive or moving average terms as they are not condusive to HMMs in this context. The parameter controling the influence of the previous data point(s) is too general making it really easy for chains to find local optimums. In other words, these parameters are too flexible and don't constrain the state models enough, resulting in a lot of overlap. To account for hysteresis in the water state due to snow melt and dry summers I applied a double cycle cosine curve that draws temperature estimates down in early spring, up in the early summer, down again in early fall (i.e. rain) and up again in early winter (insulating snowpack). This seems to capture the hysteresis without allowing the model too much flexibility.

After giving the HMM some guidance using fairly general initial parameters, we get very good accordance with the known data types (>99%).

Now I will apply this model to a single year of real data and just see if it can capture the same relationships when the data may or may not follow a cosine curve perfectly. I will fully inform the air model with estimated parameters from the ClimateBC model which will constrain the parameter space for the water model and allow for better convergence.

```{r HMM on Real Data}
#Read in water temperature data.

#Daily
#df = read_rds("./Data/thompson_daily_water_manual_clean.rds") %>% 
#  filter(site==49, year(date)>2014, year(date)<2016) %>% 
#  mutate(n = n_init(date), d = yday(date))
#Hourly
df = read_rds("./Data/thompson_water.rds") %>% 
  filter(site==49, year(date)>2014, year(date)<2016) %>% 
  mutate(n = n_init(date)*2, d = (yday(date)*24) + (hour(date)-24))

#Read in ClimateBC air temperature approximations
df_inits = read_rds("./Data/thompson_air_inits.rds") %>% 
  filter(site == 49, year == 2015)

#Create initial values for the model
inits = create_inits(air_A = df_inits$air_A, air_mean = df_inits$air_mean, chains = 4)

#Read in nearest air station.
air = read_rds("./Data/thompson_air.rds") %>% filter(place == "DARFIELD") %>% 
  filter(year == 2015)

#Compile Model
compile = stan_model("./stan/02_HMM.stan")
#Run model.
mod = sampling(compile, data= list(N = nrow(df), K = 2,
                                   y = round(df$temperature,2), 
                                   n = df$n,
                                   d = df$d,
                                   air_A = df_inits$air_A,
                                   air_mean = df_inits$air_mean),
               pars = c("alpha_w","A","tau_est","snow","sigma","alpha_tk","gamma"),
               init = inits, iter = 500, chains = 4, thin = 1,
               control = list(adapt_delta = .8, max_treedepth = 10), save_warmup = T)

#Filter Plot
p_single = prob_plot(mod, df$date, df$temperature, air = air, smooth = F,
          bc_climate = list(air_mean = df_inits$air_mean, air_A = df_inits$air_A), dur = "hour")

#Smoothed Plot
prob_plot(mod, df$date, df$temperature, air = air, smooth = T,
          bc_climate = list(air_mean = df_inits$air_mean, air_A = df_inits$air_A), dur = "hour")
```

In the real data, if we don't provide air mean and amplitude estimates the model appears to split the water temperature data in two, thereby sharing the same results with only marginal differences. I suspect this could be a problem when a site has no air temperature data to fit. By estimating the amplitutde and mean annual air temperature at a site via the BC Climate module and lat./long. information, we can guarentee one model to be air temperature and the other water. We should set the priors for these values rather narrow to ensure they can only be moved by a fair bit of evidence to the contrary. After fitting the model where the vertical adjustement and amplitude of the air temperature model are clearly defined we get the temperature models fitting the data correctly and even capture the single air temperature values at the beginning and end of the dataset. Furthermore, by adding a double cycle seasonal cosine curve, I was able to capture snow effect dampening of temperature in the spring and fall rain effects that lead to histeresis in the annual cosine curve. Smoothing backward as well as forward improves state estimate certainty when data of a specific state are nearby but this also leads to mislabeleing among more sparse data.

Now we can apply our HMM to raw stream temperature data at multiple sites in the North Thompson. Originally I was thinking this model would be hierarchical but found the global models pulled the site estimates away from the data resulting in poor state probability estimation. The only differenc with the model below is it allows for multiple sites and years to be fit rather than just a single site.

```{r Thompson Watershed HMM}
#Read in water temperature data and add columns; 
# 1.) enumerating the observations continuously (i.e., d) within a year,
# 2.) describing the number of days in an annual cycle (i.e, n),
# 3.) enumerating the years from first to last (i.e., year_no),
# 4.) noting the beginning and end of each year.

#Hourly
df = read_rds("./Data/thompson_water.rds") %>% 
  filter(site %in% c(49,63,54,39,1,80,87,93,100,77)) %>% 
  mutate(day = ymd(floor_date(date, unit = "day")), n = n_init(date)*2,
         d = (yday(date)*24) + (hour(date)-24),
         year = year(date), year_no = year-(min(year)-1)) %>% 
  group_by(site, year) %>% mutate(reset = if_else(date==min(date),1,
                                                  if_else(date==max(date),2,0))) %>% ungroup
#Daily 
df = read_rds("./Data/thompson_daily_water_manual_clean.rds") %>% 
  filter(site %in% c(49,63,54,39,1,80,87,93,100,77)) %>%
  mutate(day = ymd(date), year = year(date), d = yday(date),
         year_no = year - (min(year)-1)) %>% 
  group_by(site, year) %>% mutate(reset = if_else(day==min(day),1,
                                                  if_else(day==max(day),2,0)),
                                  n_cycle = n_init(date)) %>% ungroup()

#Create a reference table to add new continuous site ID values to the water temperature data. 
ref = df %>% select(site) %>% distinct() %>% mutate(new_id = c(1:n()))

# add new site ID
df = df %>% left_join(.,ref) 

#Read in climate data, summarize by year and subset.
df_inits = read_rds("./Data/thompson_air_inits.rds") %>% 
  filter(site %in% c(49,63,54,39,1,80,87,93,100,77)) %>%  
  mutate(year_no = year - (min(year)-1))
df_inits = df %>% select(site, year) %>% distinct() %>% left_join(., df_inits)

# add flag noting data gaps for forward and backward smoothing
logic = (diff(df$day)>1 & df$reset[-1] == 0)
df[c(FALSE,logic),"reset"] = 1

#Gather initial values.
inits = create_inits(air_A = df_inits$air_A, air_mean = df_inits$air_mean, chains = 4, sites = 10)

#Add a column counting in sequence the unique pairs of sites and years.
df = df %>% select(new_id,year_no) %>% distinct() %>% mutate(rowe = c(1:nrow(.))) %>%
  left_join(df,.)

#Compile and run model.
compile = stan_model("./stan/04_HMM_MultiSite.stan")
mod = sampling(compile, data= list(N = length(df$temperature),
                                 S = length(unique(df$new_id)),
                                 site = df$new_id,
                                 R = sum(df$reset==2),
                                 rowe = df$rowe,
                                 K = 2,
                                 y = round(df$temperature,2),
                                 d = df$d,
                                 reset = df$reset,
                                 accum = which(df$reset==2),
                                 n = df$n,
                                 air_A = df_inits$air_A,
                                 air_mean = df_inits$air_mean),
               pars = c("alpha_w","A","tau_est","snow","sigma","alpha_tk"),
               init = inits, iter = 500, chains = 4,
               control = list(adapt_delta = .8, max_treedepth = 10))
write_rds(mod, "./stan/HourlyMultiSite_500_obs.rds")

shinystan::launch_shinystan(mod)


  #Read in model.
mod = read_rds(path = "./stan/Mod_Results/MultiSite_500_obs.rds")

#---- Gather & Organize Coefficients ----#

#Coefficients
cf = rstan::extract(mod)

#Create full date range.
dates_full = data.frame(day = seq.Date(from = ymd("2014-01-01"), to = ymd("2017-12-31"), by = "day"))

#Incorporate full range of dates into each site for fitting and plotting.
full = df_dT %>% mutate(xi_est = apply(cf$alpha_tk[,,1],2,FUN = quantile, probs = .5)) %>% 
  group_by(site) %>%
	do({
		temp = left_join(dates_full,.,by="day")
		temp$year = year(temp$day)
		temp$d = yday(temp$day)
		temp$n = sapply(X = temp$day, FUN = n_init)
		temp$site = unique(.$site)
		temp$new_id = unique(.$new_id)
		temp
	})
test = full %>% group_by(site,year) %>% do({
  #browser()
  row = unique(.$rowe[!is.na(.$rowe)])
  if(length(row)>0) data.frame(rowe = rep(row,length(.$rowe)))
  else data.frame(rowe = rep(NA,length(.$rowe)))
})
full$rowe = test$rowe
full = full %>% filter(!is.na(rowe))

#Temperature posteriors
rtrn = full %>% group_by(site, year_no, day) %>% do({
	temp = (cf$alpha_w[,.$rowe] + cf$A[,.$rowe,1]*cos(2*pi*.$d/.$n + cf$tau_est[,.$new_id,1]*pi)) + cf$snow[,.$rowe]*cos(2*pi*.$d/(.$n/2) + cf$tao_est[,.$new_id]*pi)
	Q = quantile(temp, probs = c(.025,.1,.5,.9,.975))
	std = quantile(cf$sigma[,.$new_id,1], probs = .5)
	data.frame(t(Q),std)
})

#Correct names
names(rtrn) = c("site","year_no","day","Q2.5","Q1","Q5","Q9","Q97","std")
rtrn = left_join(full,rtrn)

#Read in original hourly data
hourly = read_rds("./Data/thompson_water.rds") %>%
  filter(site %in% c(49,63,54,39,1,80,87,93,100,77)) %>% 
  mutate(day = ymd(floor_date(date, unit = "day")))
#Add HMM results to hourly data and determine the probability of the hourly data given the ...
# ... lognormal errors using the SD and Mean of the daily model.
hourly = rtrn %>% select(site, day, starts_with("Q"), std, xi_est, error_lab) %>% 
  left_join(hourly,.)

ggplot(hourly %>% filter(site == 49),aes(day, temperature)) +
  geom_point(aes(color = xi_est),alpha = .2) + 
  geom_ribbon(aes(x = day, ymin = Q5-1.97*std, ymax = Q5+1.97*std), fill = "grey", alpha = .5) +
  geom_line(aes(day, Q5), alpha = .5) +
  ggthemes::theme_tufte() +
  facet_wrap(~site)
```







############################################### Old #################################################


## Random Effect Model

Now I will begin playing with hierarchical based models where the data are partially pooled across sites but drawn from a distribution describing the parameter variability among sites.

```{r Random Effect Water Model}
df = multi_site(n = 20, years = 1, air_ends = F, air = F, randomize = c(F,F))
#Compile and run model.
compile = stan_model("./stan/03_water_rand.stan")
mod = sampling(compile, data= list(N = length(df$daily$temperature),
                                           S = length(unique(df$daily$site)),
                                           site = df$daily$site,
                                           y = df$daily$temperature,
                                           d = df$daily$d,
                                           tau = df$coefs$tau_approx, n = df$coefs$n_cycle,
                                           air_mean = df$coefs$air_meanT,
                                           prior_only = 0, # Note this option
                                           pars = c("A", "alpha", "tau_est", "sigma")),
                       iter = 300, chains = 4)

cf = broom::tidy(mod, pars = c("alpha_w","A","tau_est","sigma"), "median") %>%
  select(-std.error) %>% 
  mutate(site = as.numeric(stringr::str_extract_all(string = term, pattern = "\\d+",simplify = T)),
         coef = stringr::str_extract_all(term, "\\w+", simplify = T)[,1]) %>%
  select(-term) %>% spread(data = ., key = coef, value = estimate)

#Mean water temperature fit.
temp = df$daily %>% left_join(.,cf, by = "site") %>% mutate(
  mean = alpha_w+A*cos(2*pi*d/365+tau_est*pi))

#Variance around mean water temperature.
temp = temp %>% mutate(
  upper = mean + qt(p = .95, df = 3)*sigma,
  lower = mean - qt(p = .95, df = 3)*sigma)

#Plot data, mean estimate and error.
ggplot() +
  geom_ribbon(data = temp, aes(x = time, ymin = lower, ymax = upper), fill = "red", alpha = 0.3) +
  geom_point(data = temp, aes(time, temperature),alpha = .5, size = .5, color = "black") + 
  geom_point(data = temp, aes(time, mean), color = "blue", size = .5, alpha = .5) +
  geom_hline(yintercept = 0) +
  facet_wrap(~site) + ggthemes::theme_tufte()

#Global Model Fit
df$coefs %>% select(site, air_meanT, water_meanT) %>% left_join(.,cf) %>% 
  ggplot(data = ., aes(water_meanT, alpha_w)) + geom_point() + geom_abline(slope = 1, intercept = 0)
```

Above we use our global model to describe the relationship between mean air temperature (known) and mean water temperature (unknown). This relationship assumes lotic environments never experience mean annual temperatures below 0ºC. It also assumes that as mean air temperatures rise, mean water temperature lags behind initially until some inflection point where they become essentially the same over the middle range of air temperatures (i.e., ~20ºN to 45ºN). At really hot mean annual air temperatures (27ºC near the equator), water means again lag behind due to ground temperature dampening. By creating this relationship we can leverage known mean air temperatures across sites and draw the mean annual water temperaure towards a global mean thereby resisting local water temperature models over-fitting erroneous data (i.e., air or ground). This relationship also ensures that as we add additional sites, the predictive ability of the model gets stronger but is a bit more prone to error for sites nearing extreme values which include unusually warm and cold sites relative to the mean air temperature. Ultimately, approximating the mean annual air temperature and it's relationship with mean annual water temperature will enhance the ability for the model to discern between air, water and other erroneous data.

In the model above we demonstrate that we can generate simulated data and retrieve those values across multiple sites. We can also demonstrate that fits improve with the number of sites included. This is true even if we limit our sites latitudes between 40 and 60ºN which means that while the global model is not exploring the entire range of data we can still approximate these values pretty well.

## HMM Random Effect Model

Next we need to incorporate the hierarchical model into the HMM model and retrieve simulated data.

```{r Simulation Hierarchical HMM}
#Create data for multiple sites.
df = multi_site(n = 20, years = 2, air_ends = F, air = F, randomize = c(F,F))

#Gather initial values.
inits = create_inits(air_A = df$coefs$air_A, air_mean = df$coefs$air_meanT,
                     tau =  df$coefs$tau_approx, chains = 4)

#Compile and run model.
compile = stan_model("./stan/04_HMM_Global.stan")
mod = sampling(compile, data= list(N = length(df$daily$temperature),
                                 S = length(unique(df$daily$site)),
                                 site = df$daily$site,
                                 K = 2,
                                 y = round(df$daily$temperature,2),
                                 d = df$daily$d,
                                 reset = df$daily$reset,
                                 accum = which(df$daily$reset==2),
                                 tau = df$coefs$tau_approx,
                                 n = df$daily$n,
                                 air_A = df$coefs$air_A,
                                 air_mean = df$coefs$air_meanT,
                                 water_A = inits[[1]]$A[,1],
                                 water_mean = inits[[1]]$alpha_w),
               pars = c("alpha_w","A","tau_est","snow_w","sigma",
                        "alpha_tk","gamma",
                        "b_alpha_w", "m_alpha_w"),
               init = inits, warmup = 500, iter = 1000, chains = 4, thin = 1,
               control = list(adapt_delta = .8, max_treedepth = 10), save_warmup = F)

shinystan::launch_shinystan(mod)

#Read in saved model and data-frame.
df = read_rds("./stan/Mod_Results/04_HMM_Global_500_df.rds")
mod = read_rds("./stan/Mod_Results/04_HMM_Global_500.rds")

#Coefficients
cf = rstan::extract(mod, pars = c("alpha_w","A","tau_est","snow_w","alpha_tk","gamma"))

#Site alpha and A summary/join with known values and priors
alpha = data.frame(t(apply(cf$alpha_w, 2, `quantile`, probs = c(0.025,.25,0.5,.75,.975)))) %>% mutate(param = "alpha[w]")
A = data.frame(t(apply(cf$A[,,1], 2, `quantile`, probs = c(0.025,.25,0.5,.75,.975)))) %>% mutate(param = "A[w]")
global = df$coefs %>% select(water_meanT, water_A) %>% gather(key = obs_param, value = observed) %>% select(-1)
global = bind_rows(alpha,A) %>% rename(lower = X2.5., lower_mid = X25., fit = X50., upper_mid = X75., upper = X97.5.) %>% bind_cols(.,global) %>% mutate(prior = c(inits[[1]]$alpha_w, inits[[1]]$A[,1]))


p = ggplot(global, aes(observed, fit)) + 
  #geom_errorbar(aes(x = observed, ymax=upper, ymin=lower), linetype = 1, size = 1, alpha = .3) +
  #geom_errorbar(aes(x = observed, ymax=upper_mid, ymin=lower_mid), linetype = 1, size = 1.5, alpha = .5) +
  geom_point(col = "#45062e", shape = 19, size = .8) +
  geom_point(aes(observed, prior),shape = 1, col = "#2b6aff", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, size = .3) +
  ggthemes::theme_tufte() +
  theme(strip.text = element_text(size=10),
        axis.text = element_text(size = 8), 
        axis.title = element_text(size = 9)) +
  labs(x = expression("Simulated Temperature ("*degree*"C)"), 
       y = expression("Estimated Temperature ("*degree*"C)")) +
  facet_wrap(~param, labeller = label_parsed)
ggsave(filename = "Global_Sim.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 7.5, height = 3.5, units = "in", device = "pdf")


#ORGANIZE HMM RESULTS

#Mean temperature fit and source probabilities.
fit = df$daily %>% ungroup() %>% mutate(row = 1:length(site)) %>% group_by(site,time) %>% do({
  filter_water = quantile(cf$alpha_tk[,.$row,1], probs = c(0.025,.25,0.5,.75,.975))
  smooth_water = quantile(cf$gamma[,.$row,1], probs = c(0.025,.25,0.5,.75,.975))
  water = cf$alpha_w[,.$site] + cf$A[,.$site,1]*cos(2*pi*.$d/.$n + cf$tau_est[,.$site,1]*pi) + 
    stream_snow(.$n, .$d, cf$tau_est[,.$site,1])*cf$snow_w[,.$site]
  
  air = df$coefs$air_meanT[.$site] + df$coefs$air_A[.$site]*cos(2*pi*.$d/.$n + cf$tau_est[,.$site,2]*pi)
  temp = data.frame(t(c(quantile(water, probs = c(0.025,.25,0.5,.75,.975)),
                 quantile(air, probs = c(0.025,.25,0.5,.75,.975)),
                 filter_water, smooth_water)))
  names(temp) = c("w_lower","w_lower_mid","w_fit","w_upper_mid","w_upper",
                "a_lower","a_lower_mid","a_fit","a_upper_mid","a_upper",
                "fp_lower","fp_lower_mid","fp_fit","fp_upper_mid","fp_upper",
                "sp_lower","sp_lower_mid","sp_fit","sp_upper_mid","sp_upper")
  temp
})

fit = left_join(df$daily, fit)

#Add viridris color to the dataframe.
ref = data.frame(ref=seq(0,1,by=0.01), col = viridis::viridis(100+1, direction = -1, alpha = 0.1))
fit$col = unlist(apply(fit,1,function(x){
  as.character(ref[round(ref$ref,2)==round(as.numeric(x["fp_fit"][[1]]),2),"col"][1])
}))
fit$p_loc = 23

#Calculate error rates for full range of possible outcomes using both filtered and smoothed probabilities.
perc = seq(.5,.99,.001)
typeI = data.frame(t(sapply(X = perc, FUN = function(t){
  apply(fit[,18:28],2, function(x){sum(x>t & fit[,2] != "water")})
}))) %>% mutate(errorType = "typeI", percent = perc*100)

typeII = data.frame(t(sapply(X = perc, FUN = function(t){
  apply(fit[,18:28],2, function(x){sum(x<t & fit[,2] == "water")})
}))) %>% mutate(errorType = "typeII", percent = perc*100)

error = bind_rows(typeI, typeII)

#Plot error rates
p1 = ggplot(error) + 
  geom_line(data = error %>% group_by(percent) %>% summarise(total = sum(fp_fit)),
            aes(percent, total), linetype = 3) +
  geom_ribbon(aes(percent, ymin = fp_lower, ymax = fp_upper, fill=errorType), alpha = .08) +
  geom_ribbon(aes(percent, ymin = fp_lower_mid, ymax = fp_upper_mid, fill=errorType), alpha = .08) +
  geom_line(aes(percent,fp_fit,col=errorType), linetype = 1) + 
  scale_y_log10(breaks = c(10,100,1000), labels = c("10","100","1000"), limits = c(10,1000)) +
  scale_color_manual(values = c("#45062e","#2b6aff"), breaks = c("typeI","typeII"),
                     labels = c("type I","type II"), name = "Filtered Probabilities") +
  scale_fill_manual(values = c("#45062e","#2b6aff")) +
  ggthemes::theme_tufte() +
  theme(legend.direction = "horizontal", legend.position = "top",
        title = element_text(size=8),
        strip.text = element_text(size=8),
        axis.text = element_text(size = 8), 
        legend.title = element_text(size = 10),
        legend.title.align = .5) +
  guides(fill = F, colour = guide_legend(title.position = "top")) +
  labs(y = "Number of Errors", 
       x = "Water Certainty Cutoff (%)")

p2 = ggplot(error) + 
  geom_line(data = error %>% group_by(percent) %>% summarise(total = sum(sp_fit)),
            aes(percent, total), linetype = 3) +
  geom_ribbon(aes(percent, ymin = sp_lower, ymax = sp_upper, fill=errorType), alpha = .08) +
  geom_ribbon(aes(percent, ymin = sp_lower_mid, ymax = sp_upper_mid, fill=errorType), alpha = .08) +
  #geom_line(aes(percent,fp_fit,col=errorType), linetype = 1) + 
  geom_line(aes(percent,sp_fit,col=errorType), linetype = 2) + 
  scale_y_log10(breaks = c(10,100,1000), labels = c("10","100","1000"), limits = c(10,1000)) +
  scale_color_manual(values = c("#45062e","#2b6aff"),breaks = c("typeI","typeII"),
                     labels = c("type I","type II"), name = "Smoothed Probabilities") +
  scale_fill_manual(values = c("#45062e","#2b6aff")) +
  ggthemes::theme_tufte() +
  theme(legend.direction = "horizontal", legend.position = "top",
        title = element_text(size=8),
        strip.text = element_text(size=8),
        axis.text = element_text(size = 8),
        axis.title.y.left = element_blank(),
        legend.title = element_text(size = 10),
        legend.title.align = .5) +
  guides(fill = F, colour = guide_legend(title.position = "top")) +
  labs(y = "Number of Errors", 
       x = "Water Certainty Cutoff (%)")
p = gridExtra::grid.arrange(p1,p2,layout_matrix = matrix(data = c(1,2), nrow = 1, ncol = 2))
ggsave(filename = "Error_Type.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 7.5, height = 3.5, units = "in", device = "pdf")

#Determine the lowest certainty cut-off that minimizes both error types
cutoff = error %>% select(percent, fp_fit, errorType) %>% 
  spread(data = ., key = errorType, value = fp_fit) %>% 
  mutate(dif = abs(typeI-typeII)) %>% 
  filter(dif == min(dif)) %>% 
  .$percent %>% min()/100

#Add errors based on cutoff value.
fit = fit %>% 
  mutate(s_est_source = if_else(sp_fit>=cutoff,"water",if_else(sp_fit<cutoff & sp_fit>.5,"uncertain","air")),
         f_est_source = if_else(fp_fit>=cutoff,"water",if_else(fp_fit<cutoff & fp_fit>.5,"uncertain","air")),
         s_error = if_else(s_est_source == source,1,0),
         f_error = if_else(f_est_source == source,1,0))

#Create error labels for each site.
percent_error = fit %>% group_by(site) %>%
  do({
    n_water = sum(.$source == "water")
    n_AG = sum(.$source != "water")
    
    error1 = round(sum(.$source[.$fp_fit>cutoff]!="water")/n_AG*100)
    error2 = round(sum(.$source[.$fp_fit<cutoff]=="water")/n_water*100)
    data.frame(n_water, n_AG, error1, error2,
      x_pos = min(fit$time),
      y_pos = max(fit$temperature)
    )
  }) %>% 
  mutate(label = paste("type I: ",as.character(error1),"% of ",as.character(n_AG)," -- type II: ",as.character(error2),"% of ",as.character(n_water),sep=""))

labels = c("1" = percent_error$label[1],
           "2" = percent_error$label[2],
           "3" = percent_error$label[3],
           "4" = percent_error$label[4],
           "5" = percent_error$label[5],
           "6" = percent_error$label[6],
           "7" = percent_error$label[7],
           "8" = percent_error$label[8],
           "9" = percent_error$label[9],
           "10" = percent_error$label[10],
           "11" = percent_error$label[11],
           "12" = percent_error$label[12],
           "13" = percent_error$label[13],
           "14" = percent_error$label[14],
           "15" = percent_error$label[15],
           "16" = percent_error$label[16],
           "17" = percent_error$label[17],
           "18" = percent_error$label[18],
           "19" = percent_error$label[19],
           "20" = percent_error$label[20])


#Plot the probabilites as a continuous colored ribbon at the top of the error plot. This will combine the two nearly identical plots without loosing the information.

#Plot data, mean estimate and error.
p = ggplot(data = fit) +
  geom_line(aes(time, a_fit), size = .3) +
  geom_line(aes(time, w_fit), size = .3) +
  geom_ribbon(aes(x = time, ymin = w_lower, ymax = w_upper), alpha = .3, fill = "#5b0982") +
  geom_ribbon(aes(x = time, ymin = a_lower, ymax = a_upper), alpha = .2, fill = "#7eb795") +
  geom_point(aes(time, temperature, color = fp_fit), size = .5) +
  geom_hline(yintercept = 0, size = .2, linetype = 2) +
  annotate("segment", x=min(fit$time), xend = max(fit$time), y=-Inf, yend=-Inf, size = .2) +
  viridis::scale_color_viridis(direction = -1) +
  ggthemes::theme_tufte() +
  theme(legend.direction = "horizontal", legend.position = "top", 
        legend.key.width = unit(3,"cm") ,
        legend.key.height = unit(.20,"cm"),
        strip.background = element_blank(),
        strip.text.x = element_blank()) +
  scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
  labs(color ="Probability Water", x = "Month", y = expression("Temperature ("*degree*"C)")) +
  guides(colour = guide_colourbar(title.position="top")) +
  facet_wrap(~site, scales = "free_x")+
  geom_text(data = percent_error, aes(x = x_pos, y = y_pos, label = site), hjust = "left", size = 2.3)
ggsave(filename = "Probs_Sim.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "pdf")
ggsave(filename = "Probs_Sim.png", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "png", dpi = 500)

p = ggplot(data = fit, aes(time,temperature)) +
  #geom_ribbon(aes(time, ymin = w_lower, ymax = w_upper), alpha = .3, fill = "#5b0982") +
  #geom_ribbon(aes(time, ymin = a_lower, ymax = a_upper), alpha = .2, fill = "#7eb795") +
  geom_point(aes(time, p_loc), color = fit$col, shape = 15, size = .8) +
  geom_point(aes(color = f_est_source), size = 0.5) +
  geom_line(aes(time, w_fit), size = .3) +
  geom_line(aes(time, a_fit), size = .3) +
  geom_rug(data = filter(fit, f_error == 0), aes(time, temperature), sides = "b") +
  scale_color_manual(values = c("water" = "#2b6aff", "air" = "#ffc021", "uncertain" = "#45062e")) +
  geom_hline(yintercept = 0, size = .2, linetype = 3) +
  labs(color = "Estimated Thermal Source", x = "Month", y = expression("Temperature ("*degree*"C)")) +
  annotate("segment", x=min(fit$time), xend = max(fit$time), y=-Inf, yend=-Inf, size = .2) +
  ggthemes::theme_tufte() + 
  theme(legend.direction = "horizontal", legend.position = c(.65,1.085), plot.margin = unit(c(4,1,1,1),"lines"),
        strip.text = element_text(size=8),
        axis.text = element_text(size = 8)) + 
  scale_x_datetime(date_breaks = "4 month", date_labels = "%b") +
  scale_y_continuous(limits = c(-8,23)) +
  guides(color = guide_legend(title.position = "top", title.hjust = 0.8)) +
  facet_wrap(~site, labeller = as_labeller(labels)) +
  geom_text(data = percent_error, aes(x = x_pos, y = y_pos, label = site), hjust = "left", size = 2.3)
ggsave(filename = "Error_Plot.png", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "png", dpi = 500, bg = "transparent")

#Statistics
overall = 1-sum(fit$s_error)/nrow(fit)
by_site = fit %>% group_by(site) %>% summarise(error_rate = (1-(sum(s_error)/n()))*100,
                                     n = n())
mean(by_site$error_rate)
.0452*365
.007 *365

head(error)

```



```{r Thompson Watershed HMM}
#---- Gather & Organize Coefficients ----#

#Coefficients
cf = rstan::extract(mod, pars = c("alpha_w","A","tau_est","snow_w","alpha_tk","gamma"))

#Site alpha and A summary/join with known values and priors
alpha = data.frame(t(apply(cf$alpha_w, 2, `quantile`, probs = c(0.025,.25,0.5,.75,.975)))) %>% mutate(param = "alpha[w]")
A = data.frame(t(apply(cf$A[,,1], 2, `quantile`, probs = c(0.025,.25,0.5,.75,.975)))) %>% mutate(param = "A[w]")
global = df_inits %>% select(air_mean, air_A) %>% gather(key = obs_param, value = observed)
global = bind_rows(alpha,A) %>% rename(lower = X2.5., lower_mid = X25., fit = X50., upper_mid = X75., upper = X97.5.) %>% bind_cols(.,global) %>% mutate(prior = c(inits[[1]]$alpha_w, inits[[1]]$A[,1]))

#---- Gather and Spread Global Parameter Estimates ----#
global_pa = rstan::extract(mod, pars = c("m_alpha_w","b_alpha_w"))
global_pA = rstan::extract(mod, pars = "A")
#---- End Gather Site Coefficients ----#


#---- Fit and Plot Global Models ----#
air = seq(min(df_inits$air_mean), max(df_inits$air_mean), by = 0.1)
fit = data.frame(t(sapply(air, FUN = function(x){
  quantile(exp(global_pa$b_alpha_w + global_pa$m_alpha_w * x), probs = c(0.025,.25,0.5,.75,.975))
})))
fit = fit %>% rename(lower = X2.5., lower_mid = X25., fit = X50., upper_mid = X75., upper = X97.5.) %>% mutate(air = air)

fit_A = data.frame(t(apply(global_pA$A, 2, function(x){
  quantile(x[,1], probs = c(0.025,.25,0.5,.75,.975))
})))
fit_A = fit_A %>% rename(lower = X2.5., lower_mid = X25., fit = X50., upper_mid = X75., upper = X97.5.) %>% 
  mutate(water = global[global$param == "alpha[w]","fit"])


#---- Plot Alpha_a vs. Alpha_a ----#
gp1 = ggplot() +
  geom_ribbon(data = fit, aes(x = air, ymin = lower, ymax = upper), alpha = 0.08) +
  geom_ribbon(data = fit, aes(x = air, ymin = lower_mid, ymax = upper_mid), alpha = 0.05) +
  geom_point(data = global %>% filter(param == "alpha[w]"),
             aes(observed, prior), color = "royalblue", alpha = .2) +
  geom_point(data = global %>% filter(param == "alpha[w]"), aes(observed, fit), 
             col = "#45062e", shape = 19, size = .8) +
  geom_line(data = fit, aes(air, fit), linetype = 2, size = .3) +
  ggthemes::theme_tufte() +
  theme(strip.text = element_text(size=10),
        axis.text = element_text(size = 8), 
        axis.title = element_text(size = 9)) +
  labs(x = expression("Mean Annual Air Temperature ("*degree*"C)"), 
       y = expression("Mean Annual Water Temperature ("*degree*"C)"))

#---- Plot Alpha_w vs. A_w ----#
gp2 = ggplot(data = fit_A) +
  geom_errorbar(aes(x = water, ymax=upper, ymin=lower), linetype = 1, size = .1, alpha = .3) +
  geom_errorbar(aes(x = water, ymax=upper_mid, ymin=lower_mid), linetype = 1, size = .3, alpha = .5) +
  geom_point(aes(water,fit),col = "#45062e", shape = 19, size = .8)+
  geom_abline(slope = 1, intercept = 0, linetype = 2, size = .3) +
  ggthemes::theme_tufte() +
  theme(strip.text = element_text(size=10),
        axis.text = element_text(size = 8), 
        axis.title = element_text(size = 9)) +
  labs(x = expression("Mean Annual Water Temperature ("*degree*"C)"), 
       y = expression("Annual Water Temperature Amplitude ("*degree*"C)"))

p = gridExtra::grid.arrange(gp1, gp2, layout_matrix = matrix(data = c(1,2), nrow = 1, ncol = 2))
ggsave(filename = "Global_Obs.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 7.5, height = 3.5, units = "in", device = "pdf")
#---- End Fit and Plot Global Models ----#


#---- Fit Individual Site Models ----#

#Get state probability estimates and add to observed temperature dataframe.
xi_est = broom::tidyMCMC(mod, pars = "gamma", estimate.method = "median", conf.int = F)
#Extract row number of each state matrix linked to each observation.
day = as.numeric(unlist(stringr::str_extract_all(string = xi_est$term, pattern = "\\d+(?=,)")))
#Extract model type
model = unlist(apply(xi_est, 1, function(x){
  if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,2\\]")) "air"
  else if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,1\\]")) "water"
}))
df_dT = xi_est %>% mutate(model,day) %>% select(-term, -std.error) %>% 
  spread(data = ., key = model, value = estimate) %>% select(-day) %>% 
  bind_cols(df_dT,.)

#Add back value below 1ºC.
df_dT = df_dT %>% select(site, year, doy, air, water) %>% left_join(df_dT_orig,.)

#Read in air temperature data and bind to water temperature data
air_obs = read_rds("./Data/thompson_air.rds") %>% group_by(place, date) %>% 
  summarise(air_temperature = mean(mean_temp_c, na.rm = T))
air_stream = read_csv("./Data/stream_to_air_geo.csv") %>% select(site, air_site) %>% distinct()
air_stream$air_site = toupper(air_stream$air_site)
df_dT = left_join(df_dT,air_stream, by = "site")
df_dT$day = ymd(df_dT$day)
df_dT = left_join(df_dT,air_obs, by = c("air_site"="place","day"="date"))

#Create full date range.
dates_full = data.frame(day = seq.Date(from = min(as.Date(df_dT$day)), to = max(as.Date(df_dT$day)), by = "day")) 

#Incorporate full range of dates into each site for fitting and plotting.
df_dT = df_dT %>% group_by(site) %>% do({
  temp = left_join(dates_full,.,by="day")
  day_one = min(temp[temp$d==1 & !is.na(temp$d),"day"])
  temp$d = as.numeric((temp$day - day_one) + 1)
  temp$year = year(temp$day)
  temp$site = unique(.$site)
  temp$doy = yday(temp$day)
  temp$new_id = unique(.$new_id)
  temp
})

#Function for fitting full bayes site models
fit_bayes = function(df,df_init=NULL, mod, source = "water"){
  df_dT %>% group_by(new_id) %>% do({
    site = unique(.$new_id)
    if(source == "water"){
      alpha = rstan::extract(mod, "alpha_w")$alpha_w[,site]
      A = rstan::extract(mod, "A")$A[,site,1]
      tau = rstan::extract(mod, "tau_est")$tau_est[,site,2]
      snow = rstan::extract(mod, "snow_w")$snow_w[,site]
    } else {
      alpha = df_init$air_mean[site]
      A = df_init$air_A[site]
      tau = rstan::extract(mod, "tau_est")[[1]][,site,1]
    }
    data.frame(t(sapply(.$d, FUN = function(x){
      if(source == "water"){
        deriv = -sin(2*pi*x/365 + tau*pi)
        spring = if_else(deriv>0,deriv,0) * snow
        quantile((alpha + A*cos(2*pi*x/365 + tau*pi)) - spring, probs = c(0.025,.25,0.5,.75,.975))
      } else {
        quantile(alpha + A*cos(2*pi*x/365 + tau*pi), probs = c(0.025,.25,0.5,.75,.975))
      }
  }))) %>% rename(lower = X2.5., lower_mid = X25., fit = X50., upper_mid = X75., upper = X97.5.)
  }) %>% bind_cols()
}
#Gather water and air temperature estimates.
water = fit_bayes(df = df_dT, mod = mod)
air = fit_bayes(df = df_dT, df_init = df_inits, mod = mod, source = "air")
#Rename columns
names(water) = paste("water",names(water),sep = "_")
names(air) = paste("air",names(air),sep = "_")
#Combine data and fits
df = bind_cols(df_dT, water, air)

#Sample 10 sites and plot
s = c(49,65,54,39,1,80,87,93,100,74)
x_ext = range(df_dT$day)
p_df = df %>% filter(new_id %in% s)
label = p_df %>% select(new_id, site) %>% distinct()
p = ggplot(data = p_df) + 
  geom_point(aes(day, air_temperature),color = "red", alpha = .1, shape = 1, size = .3) +
  geom_ribbon(aes(x=day,ymin=air_lower,ymax=air_upper), alpha = .1, fill = "#7eb795") +
  geom_ribbon(aes(x=day,ymin=air_lower_mid,ymax=air_upper_mid), alpha = .3, fill = "#7eb795") +
  geom_ribbon(aes(x=day,ymin=water_lower,ymax=water_upper), alpha = .1, fill = "#5b0982") +
  geom_ribbon(aes(x=day,ymin=water_lower_mid,ymax=water_upper_mid), alpha = .3, fill = "#5b0982") +
  geom_point(aes(day, temperature, color = water), shape = 19, size = .3) +
  geom_hline(yintercept = 0, size = .2, linetype = 2) +
  annotate("segment", x=x_ext[1], xend = x_ext[2], y=-Inf, yend=-Inf, size = .2) +
  viridis::scale_color_viridis(direction = -1) +
  ggthemes::theme_tufte() +
  scale_x_date(date_breaks = "6 month", date_labels = "%y-%m") +
  labs(color ="Probability Water", x = "Year-Month", y = expression("Temperature ("*degree*"C)")) +
  theme(legend.direction = "horizontal", legend.position = "top", 
        legend.key.width = unit(3,"cm") ,
        legend.key.height = unit(.20,"cm"),
        strip.background = element_blank(),
        strip.text.x = element_blank()) +
  guides(colour = guide_colourbar(title.position="top")) +
  ylim(-25,25) +
  facet_wrap(nrow = 5, ncol = 2, facets = ~new_id) +
  geom_text(data = label, aes(x = x_ext[1], y = 23, label = site), hjust = "left", size = 2.3)
#ggsave(filename = "Probs_Obs.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "pdf")

p_full = gridExtra::grid.arrange(p, p_single[[1]], p_single[[2]], p_single[[3]], 
                        layout_matrix = matrix(data = c(rep(1,10),2,3,3,4,4), nrow = 5, ncol = 3))
ggsave(filename = "Probs_Obs_Full.pdf", path = "~/Documents/rTDataScrub/images", plot = p_full, width = 13, height = 8.5, units = "in", device = "pdf")
```

After much tinkering we have chosen to linearize the global model described in the water only random effect model (i.e.,"water_rand.stan"). The model describing the steapness of the relationship seemed much too sensative, biasing mean water temperature estimates in the lower exponential part of the sigmoidal curve. We also included an amplitude global model that is approximately linear with the mean water temperature estimates. This model works for northern climate locations but I suspect will fail as we get closer to the equator where water temperature never nears 0ºC but rather are upper bound. This aspect of the model will be left for future development at this time. I've also discovered the importance of narrow priors on air and ground models and looser priors on water model parameters. By doing this we put the focus on estimating water parameters and less weight on partially known values such as air parameters. We expect that the data this model receives will mostly be water with limited air and often no ground temperature data. As such we want to make sure the data are primarily being used to inform the water model and less so the ground and air models.

I believe that since the data are generated under a logistic curve and the model linearizes using a log transformation, it becomes difficult to not have bias in the global model where lower mean annual water temperature data is over estimated while larger values are under-estimated. How does one allow the fit of the data take priority over fitting the global model? 

Some sites have long periods over the winter that flat line around zero and exhibit no seasonal cycle. This period of no cyclical variation leads to the model trying to fit these data by reducing the alpha_w and A values close to zero. This is particularly true at sites where water and air temperatures are very similar over the summer as the air model fits the summer data and then the water model is left to fit the winter data. We may need to address this problem as in Letcher (2016) and identify periods that don't show strong correlation with a cyclical pattern and eliminate or downweight their influence. Alternatively, this is where we need to identify a state change in the variance from high to extremely low when looking at difference data.


## Notes on Development

#### Generated Quantities Model Error
Sometimes when smoothing the data, the logbeta values are very small which leads to underflow and zeros in the beta/alpha_tk estimates. This leads to all zeros in the loggamma estimate and nonsense in the gamma estimate. For now I've just asked the model to print when this occurs. It may be a result of poorly developed models that have sense been remedied.
Example: {
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
}

#### Amplitude Global Model
An amplitdue global model would be related to mean annual temperature in some way. Mean annual temperature would have a lower bound (0ºC) and an upper bound (~27ºC). Amplitude would probably increase linearly untill some inflection point midway between the bounds. Thus the amplitude would equal the distance between the mean annual temperature and the lower bound until the mean annual temperature increases to the point where the distance between the mean annual temperature and upper bound is less than the distance between the mean annual temperature and the lower bound.

            |2*(alpha_water-w_low)    : alpha_water-w_low < alpha_max-alpha_water
amplitude = |
            |2*(alpha_max-alpha_water): alpha_water-w_low > alpha_max-alpha_water

#### Mean water temperature global model
We need a global model that describes how air and water temperature are loosely related to help the model find a fit and limit the data to physically reasonable boundaries. I propose a model where water temperature follows air temperature, often laging below air temperature but asymptoting at 0 and ~27ºC. This model is the same as a logistic curve where the parameters are related to temperature. In this model it is important to remember that air estimates will be coming from models that already account for latitude, elevation and distance to large water water bodies. Therefore, we can assume these variables are accounted for when estimating water temperature from air temperature. By strengthening priors on the air parameters we can direct change to the water temperature part of the model. This will correct the influence between what is known prior to the fit (partial understanding of air) and what is largely unknown (water parameters).

#### By hand corrections.
Site 17 might need extra attention as it is the site with potential ground influence.
Site 79ish has some odd cold temperatures in the summer.

#### Flexible cosine model Idea

- This is likely just another form of a moving average model upon reflection. Once the data are clean, a MA model would be useful for filling gaps in data.

To make the water cosine model more flexible would be to adjust the d-parameter by the error in the observed data. In this way we can slow down or excelerate the curves progression over time but retain some rigidity to the model so it wont accomodate all data like a traditional MA model. Basically, we'd...
  1. Calculate the difference between the observed and the mean err = (obs-mea).
  2. Divide the error by the instantanious rate (i.e., derivative, A\*-sin(x)) d_adj = err/rate.
  3. Add the result in d_adj to the current d to create d in the next time step.
I'm not sure this solves the flexibility problem. I'm worried that when fitting the model the alpha and A parameters will just expand to encompass all the data and then use this rate adjustment to account for errors in the model.

```{r Flexible Cosine}
site = df_dT %>% filter(site == 49)
site$mean = 7.4 + 7.7*cos(2*pi*site$d/site$n_cycle + 0.07962798 *pi)

a = 7.4; A = 7.7
fin = vector(mode = "numeric", length = length(site$mean))
for(i in site$d){
  #browser()
  if(i == 1){
    est = a+A*cos(2*pi*i/365 + 0.07962798 *pi)
    d = 1
  } else est = a+A*cos(2*pi*d/365+ 0.07962798 *pi)
  err = site$temperature[i] - est
  rate = -sin(2*pi*d/365)
  if(i == 1) d = i + rate*err
  else d = d + err/rate
  fin[i] = est
}
site$fin = fin
ggplot(site, aes(day, temperature)) + geom_point() + geom_line(aes(day,mean),col = "blue") + 
  geom_line(aes(day, fin),col = "red")

```

#### Post Processing Idea
After a mean and amplitude are identified across sites we should be able to normalize the data by the mean and then ask by day across sites if any data points stand out away from the group. Data points that are in the tails of a say a broad t-distribution could be given a probability that down weights the liklihood of being water data. Also, if data across sites are commonly beyond the seasonal cosine curve at some point in the year, then we can improve their liklihood of being water.

Ultimately, if we scale the data by removing the mean trend from the raw data (in my case it would be hourly), we can then run a secondary model that looks for exteme changes in variance by site and considers the liklihood of the data across the sites at the daily level. In this way we should be able to get quite exact with the results.
