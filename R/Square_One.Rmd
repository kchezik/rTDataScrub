---
title: "Square_One"
author: "Kyle Chezik"
date: "5/18/2018"
output: html_document
---

```{r StartUp Stan}
#Get necessary libraries.
library(tidyverse); library(rstan)
#Initiate cores and make sure unchanged models are not recompiled.
options(mc.cores = parallel::detectCores(), auto_write = TRUE)
```

##Data Simulation and Parameter Estimation Functions

These functions create simulated data and offer tools to estimate parameter priors.

```{r Generative Functions}
#Logistic curve relating air and water temperature (i.e., Global Model).
alpha_est = function(){
  #k = rnorm(1,.15,.01) #Rate of change between max and min mean t.
  k = .15
  #b = rnorm(1,0,.01) #Minimum mean temperature.
  b = 0
  #alpha_max = rnorm(1,27,.1)-b #Maximum mean temperature minus the minimum mean temperature.
  alpha_max = 27
  #m = rnorm(1,16,.1) #Air temperature that is the mid point between alpha_max and b.
  m = 16
  data.frame(alpha_max, k, b, m)
}
# Create annual temperature curves.
create = function(A, AR, s_t, meanT, years = 1, tao, df = 3){
  N = years*365 * 24 # Number of samples.
  d_cycle = cos((years*2)*pi*1:N/N + tao*pi) # Generic season cycle.
  sd = 1/exp(abs(diff(d_cycle))); sd = (scale(sd)+2)/s_t; sd = c(sd[,1], sd[1,1])
  h_cycle = sd*(cos((years*2)*pi*1:N/24 + tao*pi)) # Daily cycle by hour.
  noise = arima.sim(model = list(ar = AR), n = N, 
                    rand.gen = function(n, ...) rt(n, df = df)) # Seasonally adjusted AR1 correlated noise.
  (d_cycle * A + meanT) + h_cycle + noise*sd #Return combined data.
}
# Extract chunks of data randomly within a year.
period = function(df, col){
  library(tidyverse);library(lubridate)
  init = sample_n(df, 1)$time
  fin = init  + ddays(round(psych::logistic(rnorm(n = 1, mean = psych::logit(yday(init)/365), sd = 1))*365))
  names(df)[[col]] = "temperature"
  filter(df, time >= init, time <= fin) %>% select(time, temperature)
}
# Eliminate data from the dataframe that has been used so there isn't overlap.
reduce = function(df, sub_df){
  df[!(df$time %in% sub_df$time),]
}
# This function is a wrapper that uses samp_per(), create(), ground(), period() and reduce() to generate random data over the course of a year that is made up of three different functions.
generate = function(air_ends = F, air = F, years = 1, randomize = c(T,T), global){
  library(tidyverse); library(lubridate)
  
  # Sample location latitude.
  latitude = rnorm(1,50,4) #45 to 60ºN with a mean of 50.
  
  #Air parameters. http://www-das.uwyo.edu/~geerts/cwx/notes/chap16/geo_clim.html
  air_A = rnorm(1, .4*latitude, latitude*.02) # Temperature range is latitude dependant.
  air_AR = rbeta(n = 1, shape1 = 500, shape2 = 25)
  air_meanT = 27-(latitude-16)*.65 + rnorm(1,0,.25)
  
  #Water parameters.
  water_meanT = rnorm(1,(global$alpha_max/(1+exp(-global$k*(air_meanT-global$m))))+global$b,0.1)
  
  #relationship between W_mean and W_A.
  alpha_inf = 13.5
  if(water_meanT<alpha_inf) water_A = water_meanT + rnorm(1, 0, .1)
  else water_A = (alpha_inf - (water_meanT-alpha_inf)) + rnorm(1, 0, .1)
  
  water_AR = rbeta(n = 1, shape1 = 400, shape2 = 20)
  #Seasonal Period Parameter
  tao = rbeta(1,1,1) # Randomize sampling period.
  #Create Data.
  df = data.frame(
    air= create(A=air_A, AR=air_AR, s_t=5, meanT=air_meanT, years=years, tao=tao-0.01, df=4),
    water= create(A=water_A, AR=water_AR, s_t=20, meanT=water_meanT, years=years, tao=tao+0.01, df=10),
  
    # Add a time component so the data can be sampled.
    time = seq(from = ymd_h("2000-07-15 00") + ddays(round(182.5*tao)),
               by = "hour", length.out = years*365*24))
  
  #Add air to the begining and end.
  if(randomize[1] == T & air_ends == F) {
    ans = sample(x = c(0,1), size = 1, prob = c(.1,.9))
    if(ans == 1) air_ends = T
    ans = 0
  }
  if(air_ends == T){
    ends = filter(df,time <= floor_date(min(time), unit = "day")+dhours(23) | time >= floor_date(max(time))-dhours(23)) %>% 
      select(time, air) %>% 
      rename(., temperature = air) %>% 
      mutate(source = "air")
    df = reduce(df, ends)
  }
  
  #Select air error region.
  if(randomize[2] == T & air == F) {
    ans = sample(x = c(0,1), size = 1, prob = c(.3,.7))
    if(ans == 1) air = T
    ans = 0
  }
  if(air == T){
    airT = period(df, 1) %>%
      mutate(source = "air")
    df = reduce(df, airT)
  }
  
  #Fill remaining time period with water data.
  water = df %>% select(time, water) %>% rename(., temperature = water) %>% 
    mutate(source = "water")
  
  #Combine and return a mixed dataset.
  fin = water
  if(air_ends == T) fin = suppressWarnings(bind_rows(ends, fin))
  if(air == T) fin = suppressWarnings(bind_rows(fin, airT))
  fin = arrange(fin, time)
  
  cfs = data.frame(latitude, air_tao = tao-0.01, water_tao = tao+0.01,
                   alpha_max = global$alpha_max, 
                   k = global$k, b = global$b, 
                   aw_infl = global$m,
                   air_AR,water_AR,
                   air_A, water_A,
                   air_meanT, water_meanT)
  list(dat = fin, coefs = cfs)
}

multi_site = function(n = 1, years = 1, air_ends = F, air = F, randomize = c(T,T)){
  #Create list to be filled.
  ret = list(dat = NULL, coefs = NULL)
  #Generate hourly data `n` number of sites.
  for(i in 1:n){
    #Gather global model coefficients.
    global = alpha_est()
    #Generate hourly data made from three soures, the air, the water and the saturated earth.
    df = generate(air = air, air_ends = air_ends, 
                  randomize = randomize, years = years, global = global) 
    #Add site number and period in annual cycle.
    df$dat = mutate(df$dat, site = i, d_cycle = c(1:length(time)))
    df$coefs = mutate(df$coefs, site = i)
    #Bind data and coefs into list.
    ret$dat = suppressWarnings(bind_rows(ret$dat, df$dat))
    ret$coefs = suppressWarnings(bind_rows(ret$coefs, df$coefs))
  }
  #Rename lists
  dat = list("hourly" = ret$dat, "coefs" = ret$coefs)

  #Daily summary
  ret = ret$dat %>% mutate(time = lubridate::floor_date(time, unit = "day")) %>% 
    group_by(site, source, time) %>%
    summarize(temperature = mean(temperature)) %>% 
    arrange(site, time) %>% ungroup() %>% group_by(site)
  #Remove duplicate days due to rounding errors.
  dups = ret %>% group_by(site,time) %>% summarise(c = n()) %>% filter(c>1)
  del = vector(mode = "numeric",length = length(dups)/2); c=1
  if(nrow(dups)>0){
    for (i in 1:nrow(dups)){
      rows = which(ret$time==dups[i,"time"][[1]] & ret$site == dups[i,"site"][[1]])
      del[c] = sample(rows,1)
      c=c+1
    }
  dat$daily = ret[-del,]
  } else dat$daily = ret
  
  #Add point in cycle data (`d`).
  dat$daily = dat$daily %>% group_by(site) %>% mutate(d = c(1:length(time)))
  #Add tau prior estimate and determine how many points are in an annual cycle.
  dat$coefs = dat$daily %>% group_by(site) %>% summarise(tau_approx = tau_init(time = time),
                                                         n_cycle = n_init(time = time)) %>% 
    left_join(dat$coefs,., by = "site")
  dat
}


#Determine values to constraine the search space.

#Estimate tao via the data assuming a northern hemisphere cycle.
tau_init = function(time){
  library(lubridate)
  ref = data.frame(date = yday(seq(from = ymd("2000-07-15"), by = "day", length.out = 366)),
                 tao = seq(from = 0 , to = 2, length.out = 366))
  ref[ref$date == yday(min(time)),"tao"]
}
#Provide the number of observations in a year. Will determine the number of periods.
n_init = function(time){
  library(lubridate)
  dur = as.numeric(names(sort(table(as.duration(time[2:length(time)]-time[1:(length(time)-1)])), decreasing = T)))
  total = 0; cc = 0
  while(as.duration(total)/dyears(1) < 1){
    total = (total + dur)
    cc=cc+1
  }
  cc
}
#Build initial conditions for different numbers of chains.
create_inits = function(K = 3, A_ij = c(.9,.1,.1,.9),
                        air_A, air_alpha, chains = 1, n_sites){
  #Create single chain list
  A_ij = diag(K); A_ij = if_else(A_ij ==1, .9, .1/(K-1))
  if(n_sites == 1) A = c(4,air_A)
  else A = matrix(data = c(rep(4, n_sites),air_A), nrow = n_sites, ncol = 2)
  init = list(#p_1k = matrix(data = p_1k, nrow = n_sites, ncol = K, byrow = T),
              A_ij = matrix(A_ij, nrow = K, ncol = K),
              b_alpha_w = 2, m_alpha_w = .5, b_A = 0, m_A = 1,
              A = A,
              alpha_air = air_alpha,
              alpha_water = if_else(air_alpha>0,air_alpha,2))
  if(n_sites == 1) init$A[1] = if_else(init$alpha_water<13.5, init$alpha_water, (27-init$alpha_water))
  else init$A[,1] = if_else(init$alpha_water<13.5, init$alpha_water, (27-init$alpha_water))
  #Replicate single list over n chains
  inits = list()
  for(i in 1:chains){
    inits[[i]] = init
  }
  #return inits
  inits
}
```

## Plot Model Results

These functions offer plotting capabilities after running a Hidden Markov Model.

```{r Plot Probabilities}
prob_plot = function(mod, date, temperature, n, air = NULL, smooth = T, err_perc = NULL){
  #Build dataframe
  if(is.null(err_perc)) df = as.tibble(data.frame(date,temperature)) %>% arrange(date)
  else df = as.tibble(data.frame(date,temperature,source = err_perc)) %>% arrange(date)
  
  #Coefficients
  cf = broom::tidy(mod, pars = c("alpha_water","alpha_air","A","tau_est","sigma")) %>%
    select(-std.error) %>% spread(term,estimate)
  
  #Extract log-/probability estimates and their upper and lower quartiles
  if(smooth == T) {
    xi_est = mod %>%
    as.data.frame() %>%
    select(starts_with("gamma")) %>% 
    reshape2::melt() %>% 
    group_by(variable) %>%
    summarise(lower = quantile(value, probs = .25),
              median = median(value),
              upper = quantile(value, probs = .75))
  }
  else {
    xi_est = mod %>%
    as.data.frame() %>%
    select(starts_with("alpha_tk")) %>% 
    reshape2::melt() %>% 
    group_by(variable) %>%
    summarise(lower = quantile(value, probs = .25),
              median = median(value),
              upper = quantile(value, probs = .75))
  }
  
  #Extract original "day" of dataset
  day = as.numeric(unlist(stringr::str_extract_all(string = xi_est$variable, pattern = "\\d+(?=,)")))

  #Extract model type
  model = unlist(apply(xi_est, 1, function(x){
    if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,1\\]")) "air"
    else if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,2\\]")) "water"
    else if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,3\\]")) "ground"
  }))
  
  #Calculate probability of data given each model
  xi_est = xi_est %>% mutate(day = day, est_source = model) %>% select(-variable)
  
  #Bind probabilities to raw data.
  df1 = xi_est %>% filter(est_source == "water") %>% arrange(day) %>% bind_cols(df, .)
  df2 = xi_est %>% group_by(day) %>% filter(median == max(median)) %>% 
    arrange(day) %>% bind_cols(df,.)
  
  #Add Model Fits
  df2$water_fit = cf$`alpha_water` + cf$`A[1]`*cos(2*pi*c(1:nrow(df2))/n + cf$`tau_est[2]`*pi)
  #df2$water_sd = exp(cos(2*pi*c(1:nrow(df2))/n + cf$`tau_est[2]`*pi)) + cf$`sigma[1]`
  df2$water_sd = cf$`sigma[1]`
  if(!is.null(err_perc)){
    df2 = df2 %>% mutate(err = if_else(df2$source == df2$est_source, 1,19))
    percent_error = round((1-sum(df2$source==df2$est_source)/nrow(df))*100)
  }
  
  #Build Plots
  
  #Probability plot
  p1p = ggplot(df1) + 
    geom_line(aes(date, round(median*100)), size = .3, linetype = 2) +
    geom_ribbon(aes(date, ymin = lower, ymax = upper), alpha =.3, fill = "#5b0982") +
    scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
    ggthemes::theme_tufte() +
    theme(axis.text = element_text(size = 8),axis.title = element_text(size = 9)) +
    labs(x = "", y = "% Prob. Water")
  
  #Raw/Probability
  p1 = ggplot(df1) + 
    geom_line(aes(date, temperature), size = .2) +
    geom_point(aes(date, temperature, col = median), size = .5, shape = 19) +
    geom_hline(yintercept = 0, linetype = 2, size = .2) +
    scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
    viridis::scale_color_viridis(direction = -1) +
    ggthemes::theme_tufte() +
    theme(legend.position = "top", legend.direction = "horizontal",
          axis.line.x = element_line(color="black", size = .2),
          legend.key.width = unit(1,"cm"),
          legend.key.height = unit(.20,"cm"),
          strip.text = element_text(size=8), axis.text = element_text(size = 8),
          axis.title = element_text(size = 9), legend.title = element_text(size = 8),
          legend.text = element_text(size = 8)) +
    labs(color ="Probability Water", x = "", y = expression("Temperature ("*degree*"C)")) +
    guides(colour = guide_colourbar(title.position="top"))
  
  df2$date = lubridate::ymd(df2$date)
  #Best Fit
  p1f = ggplot(df2, aes(date,temperature)) + geom_point(aes(color = est_source), size = 0.5, shape = 19)
  if(!is.null(air)){
    p1f = p1f + geom_point(data = air, aes(date, mean_temp_c),
                           color = "red", shape = 1, size = .5, alpha = .3)
  }
  if(!is.null(err_perc)){
    p1f = p1f + geom_rug(data = filter(df2, err == 19), aes(date, temperature), sides = "b")+
      ggtitle(paste(percent_error,"% error", sep = ""))
  }
  
  p1f = p1f + geom_line(aes(date, water_fit), size = .3) +
    geom_ribbon(aes(date, ymin = water_fit - qt(.95, 3)*water_sd, ymax = water_fit + qt(.95, 3)*water_sd), 
                alpha = .1, fill = "#5b0982") +
    scale_x_date(date_breaks = "3 month", date_labels = "%b") +
    scale_color_manual(values = c("water" = "#2b6aff", "air" = "#ffc021", "ground" = "#45062e")) +
    geom_hline(yintercept = 0, size = .2, linetype = 3) +
    labs(color = "Estimated Thermal Source", x = "Month", y = expression("Temperature ("*degree*"C)")) +
    ggthemes::theme_tufte() +
    theme(legend.direction = "horizontal", legend.position = "top",
      axis.line.x = element_line(color = "black", size = .2),
      strip.text = element_text(size=8), axis.text = element_text(size = 8), 
      axis.title = element_text(size = 9), legend.title = element_text(size = 8),
      legend.text = element_text(size = 8), plot.title = element_text(size = 9)) +
    guides(colour = guide_legend(title.position="top"))
  
  p = gridExtra::grid.arrange(p1p, p1, p1f, layout_matrix = matrix(data = c(1,2,2,3,3), nrow = 5, ncol = 1))
  ggsave(filename = "probPlot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 4.25, height = 6.5, units = "in", device = "pdf")
}
```

## Seasonal Cycle Temperature Model

We are using a seasonal cycle model...
   `y[t] = alpha + A*cos(Y*2*pi*w + tau*pi) + sigma_t`, 
      `sigma_t ~ (cos(2*pi*w*t + tau)+2)*sigma`,
where `alpha` represents the vertical adjustment (i.e, mean) of the cosine curve and `A` refers to its expansion (i.e., amplitude). `Y` controls the number of cycles (i.e, years) the data contain and `w` is the frequency of the cycle and equals `t`/`N` where `t` is the current time point and `N` is the total number of time points in one cycle or in this case one year. `tau` controls where in the seasonal temperature cycle we begin observing data. The errors are seasonally adjusted so they are largest during winter and summer and least in the spring and fall. We created this model similar to that on page 67 of **Time Series Analysis and Its Applications** by Robert H. Shumway and David S. Stoffer.

Now we'll recapture `alpha`, `A`, `tau`, `rho` and `sigma`.

```{r Recapture Simulated Air & Water}
#create data
df = multi_site(n = 1, years = 1, air_ends = F, air = F, randomize = c(F,F))

#Compile and run model.
compile = stan_model("./stan/01_AW_sim.stan")
mod = sampling(compile, data = list(N = length(df$daily$temperature), y = df$daily$temperature),
                warmup = 300, iter = 800, chains = 4)

#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)
df$daily = df$daily %>% mutate(y_hat = cf$alpha + cf$A*cos(2*pi*d/365 + cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)

# Plot temperature by site.
ggplot() +
  geom_ribbon(data = df$daily, aes(time, ymin = y_hat - sd, ymax = y_hat + sd), fill = "red", alpha = 0.1) +
  geom_point(data = df$daily, aes(time, temperature), color = "red", size = .8) + 
  geom_line(data = df$daily, aes(time, temperature), size = .2) + 
  geom_line(data = df$daily, aes(time, y_hat), color = "blue") + 
  geom_hline(yintercept = 0) +
  facet_wrap(~site) + ggthemes::theme_tufte()
```

In the water and air temperature case we can easily recapture the coefficients. Now we'll apply this model to some known water and air temperature data to see if they fit the data well.

```{r Real Temperature Fit}
#Real data.
dat = read_rds(path = "./Data/test_labeled.rds")
air = dat %>% filter(source == "air", !is.na(temperature)) %>% arrange(date)
water = dat %>% filter(source == "water", !is.na(temperature)) %>% arrange(date)

#air mod
mod = sampling(compile, data= list(N = length(air$temperature), y = air$temperature),
                        warmup = 300, iter = 800, chains = 4)

#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)

#collect results
air = air %>% mutate(d = c(1:length(temperature)),
                    y_hat = cf$alpha + cf$A*cos(2*pi*d/length(temperature)+ cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)

#water mod
mod = sampling(compile, data= list(N = length(water$temperature), y = water$temperature),
               warmup = 300, iter = 800, chains = 4)
               
#Coefficients
cf = broom::tidy(mod, pars = c("alpha","A","tau","sigma"), "mean") %>%
  select(-std.error) %>% spread(term,estimate)

#collect results
water = water %>% mutate(d = c(1:length(temperature)),
                    y_hat = cf$alpha + cf$A*cos(2*pi*d/length(temperature) + cf$tau*pi),
                    sd = qt(p = .95, df = 3)*cf$sigma)                       

#plot results
ggplot() +
  geom_ribbon(data = air, aes(date, ymin = y_hat - sd, ymax = y_hat + sd), fill = "red", alpha = 0.1) +
  geom_point(data = air, aes(date, temperature), color = "red", size = .8) + 
  geom_ribbon(data = water, aes(date, ymin = y_hat - sd, ymax = y_hat + sd), fill = "blue", alpha = 0.4) +
  geom_point(data = water, aes(date, temperature), color = "blue", size = .8) + 
  geom_line(data = air, aes(date, y_hat), color = "black") + 
  geom_line(data = water, aes(date, y_hat), color = "black") + 
  geom_hline(yintercept = 0) +
  ggthemes::theme_tufte()
```

The model fits the real water and air temperature data well and captures the major differences between the two temperature sources. This is encouraging that the models, though the same, may have such different parameter values they will be seperable in the HMM.

## HMM Model

Now we need to combine the air and water temperature data and build a Hidden Markov Model that pulls them apart.

```{r Recapture Simulated Multi-Sources w/HMM}
df = multi_site(n = 1, years = 1, air_ends = T, air = T, randomize = c(F,F))
inits = create_inits(air_A = df$coefs$air_A, air_alpha = df$coefs$air_meanT, chains = 4, n_sites = 1)

#Compile and run model.
compile = stan_model("./stan/02_HMM.stan")
mod = sampling(compile, data= list(N = length(df$daily$temperature), K = 2,
                                  y = df$daily$temperature,
                                  tau = tau_init(time = df$daily$time), 
                                  n = n_init(time = df$daily$time),
                                  air_A = inits[[1]]$A[2],
                                  air_mean = inits[[1]]$alpha_air,
                                  water_A = inits[[1]]$A[1],
                                  water_mean = inits[[1]]$alpha_water),
               warmup = 500, iter = 1000, chains = 4)
#Plot
prob_plot(mod, df$daily$time, df$daily$temperature, n = 365, smooth = F, err_perc = df$daily$source)
```

I did not include auto-regressive or moving average terms as they are not condusive to HMMs in this context. The parameter controling the influence of the previous data point(s) is too general making it really easy for chains to find local optimums. In other words, these parameters are too flexible and don't constrain the models enough, resulting in a lot of overlap.

After giving the HMM some guidance using fairly general initial parameters, we get very good accordance with the known data types (>99%).

Now I would like to apply this model to my real data and just see if it can capture the same relationships when the data may or may not follow a cosine curve perfectly and have such clearly distinguished variance parameters.

```{r HMM on Real Data}
#Read in ClimateBC air temperature approximations
df_inits = read_rds("./Data/thompson_air_inits.rds") %>% 
  filter(site == 49) %>% summarise(air_mean = mean(air_mean), air_A = mean(air_A))
#Read in observation data and summarize.
df_T = read_rds("./Data/thompson_water.rds") %>% filter(site == 49) %>%
  mutate(day = lubridate::floor_date(date, unit = "day"), 
         doy = lubridate::yday(date)) %>%
  group_by(day, doy) %>% summarise(temperature = mean(temperature))
#Create initial values for the model
inits = create_inits(air_A = df_inits$air_A, air_alpha = df_inits$air_mean, 
                     n_sites = 1, chains = 4, K = 2)
#Read in nearest air station.
air = read_rds("./Data/thompson_air.rds") %>% filter(place == "DARFIELD")
#Plot all data.
ggplot() +
  geom_point(data = air, aes(lubridate::ymd(date), mean_temp_c), color = "red", shape = 19, alpha = .3) +
  geom_point(data = df_T, aes(lubridate::ymd(day), temperature), shape = 1)

#Compile Model
compile = stan_model("./stan/02_HMM.stan")
#Run model.
mod = sampling(compile, data= list(N = nrow(df_T), K = 2,
                                   y = round(df_T$temperature,2),
                                   tau = tau_init(time = df_T$day), 
                                   n = n_init(time = df_T$day),
                                   air_A = inits[[1]]$A[2],
                                   air_mean = inits[[1]]$alpha_air,
                                   water_A = inits[[1]]$A[1],
                                   water_mean = inits[[1]]$alpha_water),
               init = inits, warmup = 500, iter = 1000, chains = 4)

#Plot
prob_plot(mod, df_T$day, df_T$temperature, n = n_init(time = df_T$day), air = air, smooth = F)
```

In the real data, if we don't provide air mean and amplitude estimates the model appears to split the water temperature data in two, thereby sharing the same results with only marginal differences. I suspect this could be a problem when a site has no air temperature data to fit. By estimating the amplitutde and mean annual air temperature at a site via the BC Climate module and lat./long. information, we can guarentee one model to be air temperature and the other water. We should set the priors for these values rather narrow to ensure they can only be moved by a fair bit of evidence to the contrary. After fitting the model where the vertical adjustement and amplitude of the air temperature model are clearly defined we get the temperature models fitting the data correctly and even capture the single air temperature values at the beginning and end of the dataset. 

The below model includes a smoother which increases certainty of estimates with a lot of similar data nearby but this also leads to mislabeleing among more sparse data.

```{r Smoothed HMM}
#Plot
prob_plot(mod, df_T$day, df_T$temperature, n = n_init(time = df_T$day), air = air, smooth = T)
```

## Random Effect Model

Now I will begin playing with hierarchical based models where the data are partially pooled across sites but drawn from a distribution describing the parameter variability among sites.

```{r Random Effect Water Model}
df = multi_site(n = 20, years = 1, air_ends = F, air = F, randomize = c(F,F))
#Compile and run model.
compile = stan_model("./stan/03_water_rand.stan")
mod = sampling(compile, data= list(N = length(df$daily$temperature),
                                           S = length(unique(df$daily$site)),
                                           site = df$daily$site,
                                           y = df$daily$temperature,
                                           d = df$daily$d,
                                           tau = df$coefs$tau_approx, n = df$coefs$n_cycle,
                                           air_mean = df$coefs$air_meanT,
                                           prior_only = 0, # Note this option
                                           pars = c("A", "alpha", "tau_est", "sigma")),
                       iter = 300, chains = 4)

cf = broom::tidy(mod, pars = c("alpha_w","A","tau_est","sigma"), "median") %>%
  select(-std.error) %>% 
  mutate(site = as.numeric(stringr::str_extract_all(string = term, pattern = "\\d+",simplify = T)),
         coef = stringr::str_extract_all(term, "\\w+", simplify = T)[,1]) %>%
  select(-term) %>% spread(data = ., key = coef, value = estimate)

#Mean water temperature fit.
temp = df$daily %>% left_join(.,cf, by = "site") %>% mutate(
  mean = alpha_w+A*cos(2*pi*d/365+tau_est*pi))

#Variance around mean water temperature.
temp = temp %>% mutate(
  upper = mean + qt(p = .95, df = 3)*sigma,
  lower = mean - qt(p = .95, df = 3)*sigma)

#Plot data, mean estimate and error.
ggplot() +
  geom_ribbon(data = temp, aes(x = time, ymin = lower, ymax = upper), fill = "red", alpha = 0.3) +
  geom_point(data = temp, aes(time, temperature),alpha = .5, size = .5, color = "black") + 
  geom_point(data = temp, aes(time, mean), color = "blue", size = .5, alpha = .5) +
  geom_hline(yintercept = 0) +
  facet_wrap(~site) + ggthemes::theme_tufte()

#Global Model Fit
df$coefs %>% select(site, air_meanT, water_meanT) %>% left_join(.,cf) %>% 
  ggplot(data = ., aes(water_meanT, alpha_w)) + geom_point() + geom_abline(slope = 1, intercept = 0)
```

Above we use our global model to describe the relationship between mean air temperature (known) and mean water temperature (unknown). This relationship assumes lotic environments never experience mean annual temperatures below 0ºC. It also assumes that as mean air temperatures rise, mean water temperature lags behind initially until some inflection point where they become essentially the same over the middle range of air temperatures (i.e., ~20ºN to 45ºN). At really hot mean annual air temperatures (27ºC near the equator), water means again lag behind due to ground temperature dampening. By creating this relationship we can leverage known mean air temperatures across sites and draw the mean annual water temperaure towards a global mean thereby resisting local water temperature models over-fitting erroneous data (i.e., air or ground). This relationship also ensures that as we add additional sites, the predictive ability of the model gets stronger but is a bit more prone to error for sites nearing extreme values which include unusually warm and cold sites relative to the mean air temperature. Ultimately, approximating the mean annual air temperature and it's relationship with mean annual water temperature will enhance the ability for the model to discern between air, water and other erroneous data.

In the model above we demonstrate that we can generate simulated data and retrieve those values across multiple sites. We can also demonstrate that fits improve with the number of sites included. This is true even if we limit our sites latitudes between 40 and 60ºN which means that while the global model is not exploring the entire range of data we can still approximate these values pretty well.

## HMM Random Effect Model

Next we need to incorporate the hierarchical model into the HMM model and retrieve simulated data.

```{r Simulation Hierarchical HMM}
#Create data for multiple sites.
df = multi_site(n = 20, years = 1, air_ends = T, air = F, randomize = c(F,T))

#Gather initial values.
inits = create_inits(air_A = df$coefs$air_A, air_alpha = df$coefs$air_meanT, n_sites = length(unique(df$daily$site)), chains = 4, K = 2)

#Compile and run model.
compile = stan_model("./stan/04_HMM_GWA.stan")
mod = sampling(compile, data= list(N = length(df$daily$temperature),
                                 S = length(unique(df$daily$site)),
                                 site = df$daily$site,
                                 K = 2,
                                 y = round(df$daily$temperature,2),
                                 d = df$daily$d,
                                 tau = df$coefs$tau_approx,
                                 n = df$coefs$n_cycle,
                                 air_A = inits[[1]]$A[,2],
                                 air_mean = inits[[1]]$alpha_air,
                                 water_A = inits[[1]]$A[,1],
                                 water_mean = inits[[1]]$alpha_water,
                                 prior = 1),
               init = inits, iter = 300, chains = 4,
               control = list(adapt_delta = .9, max_treedepth = 10))

shinystan::launch_shinystan(mod)

#Coefficients
cf1 = broom::tidy(mod, pars = c("A","tau_est","sigma"), "median") %>%
  select(-std.error) %>% 
  mutate(
    site = as.numeric(stringr::str_extract_all(string = term, pattern = "\\d+",simplify = T)[,1]),
    model = as.numeric(stringr::str_extract_all(string = term, pattern = "\\d+",simplify = T)[,2]),
    coef = stringr::str_extract_all(term, "\\w+", simplify = T)[,1]
  ) %>% 
  select(-term) 

cf2 = broom::tidy(mod, pars = c("alpha_a","alpha_w"), "median") %>%
  select(-std.error) %>% 
  mutate(site = as.numeric(stringr::str_extract_all(string = term, pattern = "\\d+(?=\\])",simplify = T)),
         model = c(rep(2,20),rep(1,20)),
         coef = stringr::str_extract_all(term, "\\w+", simplify = T)[,1]) %>% 
  select(-term)

cf = rbind(cf1,cf2) %>% spread(key = coef, value = estimate)

#Gather and plot global priors and estimates.
global =data.frame(observed = c(df$coefs$water_meanT,df$coefs$water_A),
                   estimated = c(cf[cf$model == 1,"alpha_w"], cf[cf$model == 1,"A"]),
                   prior = c(inits[[1]]$alpha_water,inits[[1]]$A[,1]),
                   parameter = c(rep("alpha[w]",max(cf$site)), rep("A[w]",max(cf$site))))

p = ggplot(global, aes(observed, estimated)) +
  geom_point(col = "#45062e", shape = 19, size = .8) +
  geom_point(aes(observed, prior),shape = 1, col = "#2b6aff", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, size = .3) +
  ggthemes::theme_tufte() +
  theme(strip.text = element_text(size=10),
        axis.text = element_text(size = 8), 
        axis.title = element_text(size = 9)) +
  labs(x = expression("Simulated Temperature ("*degree*"C)"), 
       y = expression("Estimated Temperature ("*degree*"C)")) +
  facet_wrap(~parameter, labeller = label_parsed)
ggsave(filename = "Global_Plot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 7.5, height = 3.5, units = "in", device = "pdf")


#ORGANIZE HMM RESULTS

#Mean water temperature fit.
temp = cf %>% filter(model == 1) %>% left_join(df$daily, ., by = "site") %>% 
  mutate(mean_w = alpha_w + A*cos(2*pi*d/365+tau_est*pi))

#Variance around mean water and ground temperature.
temp = temp %>% mutate(
  upper_w = mean_w + qt(.95,3)*sigma,
  lower_w = mean_w - qt(.95,3)*sigma) %>%
  select(-c(6:11))

#Mean air temperature fit.
temp = cf %>% filter(model == 2) %>% left_join(temp, ., by = "site") %>% 
  mutate(mean_a = alpha_a + A*cos(2*pi*d/365+tau_est*pi))

#Variance around mean air temperature.
temp = temp %>% mutate(
  upper_a = mean_a + qt(.95,3)*sigma,
  lower_a = mean_a - qt(.95,3)*sigma
  ) %>% 
  select(-c(9:14))

xi_est = mod %>%
as.data.frame() %>%
select(starts_with("alpha_tk")) %>% 
reshape2::melt() %>% 
group_by(variable) %>%
summarise(median = median(value))

#Extract original "day" of dataset
day = as.numeric(unlist(stringr::str_extract_all(string = xi_est$variable, pattern = "\\d+(?=,)")))

#Extract model type
model = unlist(apply(xi_est, 1, function(x){
  if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,1\\]")) "air"
  else if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,2\\]")) "water"
  else if(grepl(x = x[1],  pattern = "\\w+\\[\\d+,3\\]")) "ground"
}))

#Calculate probability of data given each model
xi_est = xi_est %>% mutate(day = day, est_source = model)

df1 = xi_est %>% filter(est_source == "water") %>% arrange(day) %>% bind_cols(temp, .) %>% select(-variable, -day)
df2 = xi_est %>% group_by(day) %>% filter(median == max(median)) %>% 
  arrange(day) %>% bind_cols(temp,.) %>% select(-variable, -day) %>% 
  mutate(err = if_else(source == est_source, 1,19))

perc = seq(.5,.99,.001)
typeI = vector(mode = "numeric", length = length(perc)) 
typeII = vector(mode = "numeric", length = length(perc)) 
count = 1
for(i in perc){
  df2$certainty = df2$est_source
  df2[df2$median<i & df2$est_source=="water","certainty"] = "uncertain"
  typeI[count] = sum(df2$source[df2$certainty=="water"]!="water")
  typeII[count] = sum(df2$source[df2$certainty!="water"]=="water")
  df2$certainty = df2$est_source
  count=count+1
}
cert = data.frame(percent = perc, typeI, typeII) %>%
  mutate(dif = abs(typeI-typeII)) %>% 
  gather(data = ., key = errorType, value = error, typeI, typeII)

cutoff = mean(cert[cert$dif == min(cert$dif),"percent"])

p = ggplot(cert, aes(percent*100,error,col=errorType)) + 
  geom_line() + 
  scale_y_log10() +
  scale_color_manual(values = c("#45062e","#2b6aff"),breaks = c("typeI","typeII"), labels = c("type I","type II")) +
  ggthemes::theme_tufte() +
  theme(legend.direction = "horizontal", legend.position = "top",
        strip.text = element_text(size=10),
        axis.text = element_text(size = 8), 
        legend.title = element_blank()) +
  labs(y = "Number of Errors", 
       x = "Water Certainty Cutoff (%)")
ggsave(filename = "ErrorType_Plot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 7.5, height = 3.5, units = "in", device = "pdf")

df2$certainty = df2$est_source
df2[df2$median<cutoff & df2$certainty=="water","certainty"] = "uncertain"
percent_error = df2 %>% group_by(site) %>%
  do({
    n_water = sum(.$source == "water")
    n_AG = sum(.$source != "water")
    error1 = round(sum(.$source[.$certainty=="water"]!="water")/n_AG*100)
    error2 = round(sum(.$source[.$certainty!="water"]=="water")/n_water*100)
    data.frame(n_water, n_AG, error1, error2,
      x_pos = min(df2$time),
      y_pos = max(df2$temperature)
    )
  }) %>% 
  mutate(label = paste("type I: ",as.character(error1),"% of ",as.character(n_AG)," -- type II: ",as.character(error2),"% of ",as.character(n_water),sep=""))


labels = c("1" = percent_error$label[1],
           "2" = percent_error$label[2],
           "3" = percent_error$label[3],
           "4" = percent_error$label[4],
           "5" = percent_error$label[5],
           "6" = percent_error$label[6],
           "7" = percent_error$label[7],
           "8" = percent_error$label[8],
           "9" = percent_error$label[9],
           "10" = percent_error$label[10],
           "11" = percent_error$label[11],
           "12" = percent_error$label[12],
           "13" = percent_error$label[13],
           "14" = percent_error$label[14],
           "15" = percent_error$label[15],
           "16" = percent_error$label[16],
           "17" = percent_error$label[17],
           "18" = percent_error$label[18],
           "19" = percent_error$label[19],
           "20" = percent_error$label[20])

#Plot data, mean estimate and error.
p = ggplot(data = df1) +
  geom_line(aes(time, mean_a), size = .3) +
  geom_line(aes(time, mean_w), size = .3) +
  geom_ribbon(aes(x = time, ymin = lower_w, ymax = upper_w), alpha = .3, fill = "#5b0982") +
  geom_ribbon(aes(x = time, ymin = lower_a, ymax = upper_a), alpha = .2, fill = "#7eb795") +
  geom_point(aes(time, temperature, color = median), size = .5) +
  geom_hline(yintercept = 0, size = .2, linetype = 2) +
  annotate("segment", x=min(df2$time), xend = max(df2$time), y=-Inf, yend=-Inf, size = .2) +
  viridis::scale_color_viridis(direction = -1) +
  ggthemes::theme_tufte() +
  theme(legend.direction = "horizontal", legend.position = "top", 
        legend.key.width = unit(3,"cm") ,
        legend.key.height = unit(.20,"cm"),
        strip.background = element_blank(),
        strip.text.x = element_blank()) +
  scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
  labs(color ="Probability Water", x = "Month", y = expression("Temperature ("*degree*"C)")) +
  guides(colour = guide_colourbar(title.position="top")) +
  facet_wrap(~site)+
  geom_text(data = percent_error, aes(x = x_pos, y = y_pos, label = site), hjust = "left", size = 2.3)
ggsave(filename = "Prob_Plot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "pdf")

p = ggplot(data = df2, aes(time,temperature)) +
  geom_ribbon(aes(time, ymin = lower_w, ymax = upper_w), alpha = .3, fill = "#5b0982") +
  geom_ribbon(aes(time, ymin = lower_a, ymax = upper_a), alpha = .2, fill = "#7eb795") +
  geom_point(aes(color = certainty), size = 0.5) +
  geom_line(aes(time, mean_w), size = .3) +
  geom_line(aes(time, mean_a), size = .3) +
  geom_rug(data = filter(df2, err == 19), aes(time, temperature), sides = "b") +
  scale_color_manual(values = c("water" = "#2b6aff", "air" = "#ffc021", "uncertain" = "#45062e")) +
  geom_hline(yintercept = 0, size = .2, linetype = 3) +
  labs(color = "Estimated Thermal Source", x = "Month", y = expression("Temperature ("*degree*"C)")) +
  annotate("segment", x=min(df2$time), xend = max(df2$time), y=-Inf, yend=-Inf, size = .2) +
  ggthemes::theme_tufte() + 
  theme(legend.direction = "horizontal", legend.position = "top",
        strip.text = element_text(size=8),
        axis.text = element_text(size = 8)) + 
  scale_x_datetime(date_breaks = "3 month", date_labels = "%b") +
  guides(color = guide_legend(title.position = "top", title.hjust = 1.4)) +
  facet_wrap(~site, labeller = as_labeller(labels)) +
  geom_text(data = percent_error, aes(x = x_pos, y = y_pos, label = site), hjust = "left", size = 2.3)
ggsave(filename = "Error_Plot.pdf", path = "~/Documents/rTDataScrub/images", plot = p, width = 13, height = 8.5, units = "in", device = "pdf")
      #x_pos = min(.$time[lubridate::month(.$time)==2])-30
```

Now we can apply our Hierarchical HMM to raw stream temperature data in the North Thompson. Something to consider and possibly include in our model is a random effect of A and alpha by year to adjust for inter-annual changes.

```{r Thompson Watershed Hierarchical HMM}
df_inits = read_rds("./Data/thompson_air_inits.rds") %>% 
  group_by(site) %>% summarise(air_mean = mean(air_mean), air_A = mean(air_A))
df_T = read_rds("./Data/thompson_water.rds")
df_dT = df_T %>% mutate(year = lubridate::year(date), 
                        day = lubridate::floor_date(date, unit = "day"), 
                        doy = lubridate::yday(date)) %>%
  group_by(site, year, day, doy) %>% summarise(temperature = mean(temperature))

df_inits = df_dT %>% group_by(site) %>% 
  summarise(tau = tau_init(day), n_cycle = n_init(day)) %>%
  left_join(df_inits,.)



#Gather initial values.
inits = create_inits(air_A = df_inits$air_A, air_alpha = df_inits$air_mean, 
                     n_sites = length(unique(df_inits$site)), chains = 1, K = 2)

#Compile and run model.
compile = stan_model("./stan/04_HMM_GWA.stan")
mod = sampling(compile, data= list(N = length(df_dT$temperature),
                                 S = length(unique(df_dT$site)),
                                 site = df_dT$site,
                                 K = 2,
                                 y = round(df_dT$temperature,2),
                                 d = df_dT$doy,
                                 tau = df_inits$tau,
                                 n = df_inits$n_cycle,
                                 air_A = inits[[1]]$A[,2],
                                 air_mean = inits[[1]]$alpha_air,
                                 water_A = inits[[1]]$A[,1],
                                 water_mean = inits[[1]]$alpha_water,
                                 prior = 1),
               verbose = T,
               init = inits, iter = 300, chains = 4,
               control = list(adapt_delta = .95, max_treedepth = 15))

shinystan::launch_shinystan(mod)

```

After much tinkering we have chosen to linearize the global model described in the water only random effect model (i.e.,"water_rand.stan"). The model describing the steapness of the relationship seemed much too sensative, biasing mean water temperature estimates in the lower exponential part of the sigmoidal curve. We also included an amplitude global model that is approximately linear with the mean water temperature estimates. This model works for northern climate locations but I suspect will fail as we get closer to the equator where water temperature never nears 0ºC but rather are upper bound. This aspect of the model will be left for future development at this time. I've also discovered the importance of narrow priors on air and ground models and looser priors on water model parameters. By doing this we put the focus on estimating water parameters and less weight on partially known values such as air parameters. We expect that the data this model receives will mostly be water with limited air and often no ground temperature data. As such we want to make sure the data are primarily being used to inform the water model and less so the ground and air models.

I believe that since the data are generated under a logistic curve and the model linearizes using a log transformation, it becomes difficult to not have bias in the global model where lower mean annual water temperature data is over estimated while larger values are under-estimated. How does one allow the fit of the data take priority over fitting the global model? 




## Notes on Development

#### Generated Quantities Model Error
Sometimes when smoothing the data, the logbeta values are very small which leads to underflow and zeros in the beta/alpha_tk estimates. This leads to all zeros in the loggamma estimate and nonsense in the gamma estimate. For now I've just asked the model to print when this occurs. It may be a result of poorly developed models that have sense been remedied.
Example: {
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
t: 2191 log gamma: [0,0,0] beta: [0,1,4.26337e-30] alpha_tk: [1,0,0] logbeta: [-26002.6,-16174.2,-16241.8]
}

#### Amplitude Global Model
An amplitdue global model would be related to mean annual temperature in some way. Mean annual temperature would have a lower bound (0ºC) and an upper bound (~27ºC). Amplitude would probably increase linearly untill some inflection point midway between the bounds. Thus the amplitude would equal the distance between the mean annual temperature and the lower bound until the mean annual temperature increases to the point where the distance between the mean annual temperature and upper bound is less than the distance between the mean annual temperature and the lower bound.

            |2*(alpha_water-w_low)    : alpha_water-w_low < alpha_max-alpha_water
amplitude = |
            |2*(alpha_max-alpha_water): alpha_water-w_low > alpha_max-alpha_water

#### Mean water temperature global model
We need a global model that describes how air and water temperature are loosely related to help the model find a fit and limit the data to physically reasonable boundaries. I propose a model where water temperature follows air temperature, often laging below air temperature but asymptoting at 0 and ~27ºC. This model is the same as a logistic curve where the parameters are related to temperature. In this model it is important to remember that air estimates will be coming from models that already account for latitude, elevation and distance to large water water bodies. Therefore, we can assume these variables are accounted for when estimating water temperature from air temperature. By strengthening priors on the air parameters we can direct change to the water temperature part of the model. This will correct the influence between what is known prior to the fit (partial understanding of air) and what is largely unknown (water parameters).


Exponentiating vs. adding 2 in order to make the variance positive have different outcomes in the HMM. The +2 appears to do better in the one real data scenario tested.

Site 17 might need extra attention as it is the site with potential ground influence.
