% Cleaning Stream Temperature Data with Hidden Markov Models
% ^1^Kyle A. Chezik
% ^1^Earth to Ocean Research Group -- Simon Fraser University 8888 University Dr. Burnaby BC, Canada V5A1S6 -- 778.782.9427 -- kchezik@sfu.ca

#Abstract

Data cleaning is increasingly a chokepoint in the research process as remote sensing technology facilitates mass monitoring efforts that return prodigious data volumes. Stream temperature is essential to assessing ecological processes in the lotic environment and has become relatively trivial to monitor as sensors have become small and relatively inexpensive. Here I present a semi-automated method of cleaning stream temperature data using Hidden Markov modeling (HMM), an unsupervised learning methodology that assess observation error in a probabilistic framework. Receiving raw observational data and limited prior constraints, this HMM assess the likelihood of data arising from a stream or air temperature model and bins them according to user defined certainty criteria. Using stream temperature data collected at 10 locations over 4 years in the Thompson River watershed in central British Columbia Canada, I assess this model by comparing manual efforts with HMM results at a variety of certainty thresholds. I demonstrate strong model efficacy when the air model is constrained under high certainty requirements, with as little as 2\% disagreement between manual and automated methods at some sites. Significant disagreement (27\%) occurred when stream temperature data exhibited strong spring hysteresis and overlap between air and water state estimates. In total, this model exhibited 84\% agreement with manual methods, suggesting it could prove to greatly simplify stream temperature cleaning while retaining high levels of data quality.

#Introduction

Ecological data are often difficult and expensive to collect and despite an increasing need for broad-scale ecological studies in the face of ever ubiquitous and intense human impacts [e.g., @Hughes:2017], funding for robust longitudinal and spatially extensive research is limited [@Kuebbing:2018]. In response, researchers are increasingly relying on digital remote sensing technologies to capture ecological processes over large spatial extents and at higher temporal resolution than has historically been possible (see ZSL Journal: Remote Sensing in Ecology and Conservation). For example, LiDAR is a remote sensing tool used to collect three dimensional images [e.g., @WADNR:LiDAR]. A single flight over a forest can map individual trees and be used to measure forest growth [e.g., @Caughlin:2016], canopy density [@Lee:2007], terrain topology [e.g., @WADNR:LiDAR], photosynthetic rate and even species composition [e.g., @Barbosa:2017; @Asner:2017]. LiDAR highlights the variety of ecological information that can be extracted from simple physical environmental measurements such as light and reflectance when collected at high volumes. Cost reductions and miniaturization of sensors measuring similarly fundamental physical data (e.g., temperature, light, oxygen) continuously through time, have made mass implementation easy and relatively affordable resulting in temporally resolved monitoring arrays [@Isaak:2017]. Increasingly compounding data volumes from these remote sensing technologies is facilitating an era of big data in the ecological sciences that is sure to improve our fundamental understanding of ecosystems and their management.

A current challenge in ecology is processing the massive influx of data. Although, we can now collect large quantities of data over space and time, which can return refined understandings of ecosystem processes, data errors are more difficult to detect without rigorous and time-consuming analysis. If errors in big data were strictly random occurrences we could assume they are subsumed by the data volumes themselves but because sensor technology is systematic by nature, often so are their errors, possibly leading to spurious artificial patterns. Inaccurate findings due to a lack of due diligence undermines the study, mis-informs future research and damages the credibility of science when discovered [@Liu:2016]. Data cleaning is thus requiring increasingly sophisticated approaches, with some disciplines developing clever post-processing tools that use a variety of rule-based algorithms to eliminate many common errors [e.g., @Habib:2009]. Rule-based error correction or "knowledge engineering" [@Domingos:2015], is wholly dependent on humans to identify unique error types and construct a recipe that cleans data by rigid logic. Programing logic to identify each new source of erroneous data is laborious and often leads to complexity limitations as the number of influential variables and their combinations compound. 

Increasingly the solution to the problem of compounding complexity in big data is machine learning. Neural networks, support vector machines (SVM), naive Bayes, random forest, etc., are common tools for grouping data by similar features. Rather than being hamstrung by rigid rules or parametric functions, these methods infer underlying processes and group data through feedback training, which in theory become more defined as data volumes increase. Erroneous data could constitute it's own group but often these methods need a labeled dataset that is representative of the broader population to discern clear boundaries among groups. These broadly representative datasets are not easily compiled and are often incomplete. Tools such as PCA and Isomap can group raw data without the supervision of a labeled dataset but these tools still require humans to identify groups that constitute errors. Moreover, without considering context, erroneous data can be mislabeled and computational complexity often plagues these methods as data dimensionality and volume increases. To identify errors in data, which are inherently unlabeled, we need to employ unsupervised learning methods that allow the data to be probabilistically grouped under known physical constraints. Hidden Markov Models (HMM) offer a construct that is at the intersection of human and machine intelligence, where humans provide potential conceptual models from an *a priori* understanding of the processes underlying the data and the computer bins the data under these constraints. 

Water temperature governs aquatic biological processes and is a fundamental and essential component to understanding aquatic ecology [@Angilletta:2009]. Flowing water is especially thermally dynamic and essential to understanding lotic freshwater ecosystems. Monitoring stream water temperature has become a relatively trivial and routine task due to the development of cheap, long-lived, small and environmentally robust temperature sensors [@Isaak:2017]. As a result, sensors are being deployed at increasingly high spatial and temporal resolution, resulting in exponentially growing volumes of data even for the simplest of studies [@Webb:2008]. The dynamic nature of flowing water leads to these sensors experiencing many types of error. For example, snow-derived extreme spring flows (i.e., freshet) in northern latitudes can lead to sensors being buried in sediment, blown onto river banks, tangled up in low hanging tree limbs or simply elevated in the water column due to shifts in the river bed. These scenarios are common, often go unidentified until retrieval and lead to mixed air, water and ground temperature data. These errors can be reduced by expert deployment but cannot altogether be avoided. Furthermore, human derived errors are also common and can be systematic or random. For instance, air temperature is often recorded during deployment and retrieval of sensors which is often known and recorded. Unknown is when the sensor is found by curious passersby who remove the logger from the water column resulting in random air temperature readings of various duration. Finally, the sensors themselves can experience errors such as battery failure during extreme cold periods that become resolved during warmer periods. This is a non-exhaustive accounting of errors that may or may not be obvious in a temperature time series. Not only would it be preferable if these errors were automatically identified but a machine learning approach may also eliminate the subjectivity inherent in traditional methods of data cleaning [e.g., @Sowder:2012]. Currently, it is recommended that scientists undertake a rigorous yet subjective manual data-cleaning process that involves cross comparing nearby and concurrently monitoring stream and air temperatures as well as flow data.

Here we build a Hidden Markov Model for stream temperature data with the primary goal of separating water from erroneous air temperature measurements in an effort to reduce post-processing time and human subjectivity. Constructed under a probabilistic Bayesian framework, this model assigns probabilities to data as to whether it is water temperature or erroneous air temperature.  Additionally, HMMs leverage the power of autocorrelation in time series by incorporating temporally-weighted support of adjacent data to inform categorization at the current time point. Moreover, our model leverages known physical properties of seasonal temperature cycles and modeled air temperature estimates readily available to constrain the model and improve certainty and accuracy. Combined, this model provides a framework for leveraging 'big data' to identify errors in stream temperature records and automate the increasingly laborious and time-consuming process of cleaning these data.

#Methods

*Hidden Markov Models*

Hidden Markov Models (HMM) are a type of state-space model that offers a probabilistic framework governing the transition between states through time [@Hamilton:2016]. In stream temperature data primarily two states exist, the desired state where the data exhibit stream temperature dynamics and an erroneous state where the data exhibit air temperature dynamics. The probability of being in either state is described by the first order Markov chain,

\begin{linenomath*}
\begin{equation}
	p(z_{t}|z_{t-1}), \label{eq1}
\end{equation}
\end{linenomath*}
 
where the state probabilities of air and water (*k = a, w*) at time point $t$ are informed by the state probabilities in the previous time point. This Markovian structure makes explicit an expectation that the current time point will likely remain in the same state as the previous. It is not possible to observe the true state directly so it is inferred $z_{t}$ by considering the probability of the observed data ($y_{t}$) in either state as,

\begin{linenomath*}
\begin{equation}
	p(y_{t}|z_{t}=k,\theta), \label{eq2}
\end{equation}
\end{linenomath*}

where coefficients describing each state and a transition matrix describing the global propensity to change between states are captured in $\theta$. The joint distribution of the state model $p(z_{t}|z_{t-1})$ and observation model $p(y_{t}|z_{t})$ described by,


\begin{linenomath*}
\begin{equation}
	p(z_{1:T},y_{1:T}) = [p(z_{1}) \prod_{t=2}^{T} p(z_{t}|z_{t-1})] [\prod_{t=1}^{T} p(y_{t}|z_{t})], \label{eq3}
\end{equation}
\end{linenomath*}

returns the full probability of each state given the relative support of the data in the current time step and the systems susceptibility to a state change, weighted by the state probabilities in the previous time step [@Damiano:2017]. A more in-depth discussion of the HMM model and algorithm can be found in @Hamilton:2016 and @Damiano:2017.

*States: Water & Air Temperature Models*

Water and air temperatures follow the seasonal dynamics derived from changing levels of solar radiation related to the earths orbit and tilt of its axis. The shared ultimate cause of temperature and the passive exchange of energy between the two mediums results in stream and water temperature cycles being closely correlated. But due to differences in the thermal dynamics of gases and liquids, the temperature cycles' of air and water do diverge in magnitude and timing. Therefore, I used the same cyclical model [e.g., @Shumway:2000] for both water and air temperature states described by,

\begin{linenomath*}
\begin{equation}
	y_{t} = \alpha_{k} + A_{k}\cos(2\pi\omega + \tau_{k}\pi) + \eta_{k}, \label{eq4}
\end{equation}
\end{linenomath*}

where a cosine curve capturing the seasonal oscillation of temperature ($y_{t}$) is modified by state coefficients that describe the annual dynamics of air and water temperature ($z_{w,a}$). The mean annual temperature is captured in $\alpha$ while the range of temperature values around $\alpha$ (i.e., amplitude) are captured in $A$. The frequency of the temperature cycle ($\omega$) is described by,

\begin{linenomath*}
\begin{equation}
	\omega = d_{1:\gamma}/\gamma, \label{eq5}
\end{equation}
\end{linenomath*}

where $\gamma$ is the number of observations per cycle and $d_{t}$ the integer location of observation $t$ in the cycle. I include a seasonal adjustment ($\tau$) to shift the cosine curve such that $d_{1}$ coincides with the point in the annual temperature cycle best described by the first of January, which is typically approximated by an adjustment of $\pi$ in the northern hemisphere, beginning the curve near its lowest point.

The variance around the mean temperature is a distinguishing feature between annual air and water temperature cycles. Here I assume the variance is normally distributed in both states and captured in sigma.

\begin{linenomath*}
\begin{equation}
	\eta_{k} \sim \mathrm{N}(0, \sigma_{k})  \label{eq6}
\end{equation}
\end{linenomath*}

I do not expect errors to always be independent and identically distributed because auto-correlation between data-points at the sub-daily level will not be captured by the water state model. As long as the primary components of the two states are well described, compromising the normality assumption should not impact the efficacy of state estimation.

I expect $A_{k}$, $\tau_{k}$ (eq.$\ref{eq4}$) and $\sigma_{k}$ (eq.$\ref{eq6}$) to be ordered (i.e., $w \prec a$) because water's high thermal capacity delays the response time to seasonal shifts in solar radiation and reduces the amplitude and variance of water's annual temperature cycle. Therefore, I expect the seasonality of water temperatures to lag behind air and express depressed amplitude and variance relative to the air temperature state. Ordering of variables not only captures the differential dynamics of the states but increases the stability of estimation by reducing the potential for state inversion (i.e., label switching) during the multi-chain MCMC process.

Although neither air or stream temperature profiles are perfectly described by a cosine curve, stream temperatures are particularly prone to hysteresis where spring and fall temperatures are not symetrical around summer's peak temperatures [@Letcher:2016]. Hysteresis in the annual stream temperature curve occurs for many reasons such as the high specific energy of water and snowmelt and rain [e.g., @Lisi:2015]. In northern latitudes spring snow melt depresses stream temperatures initially but waines in the summer leading to low flows and warmer stream temperatures, only to return to a cooler state as fall rains increase river discharge. To allow our stream temperature curves a modecum of seasonal flexibility, I include a double cycle cosine curve that modulates the annual curve (eq. $\ref{eq4}$), 

\begin{linenomath*}
\begin{equation}
	\mathrm{season} = \phi \cos(2\pi\omega\tfrac{1}{2}  + (\tau_{w}+c)\pi) \label{eq7}
\end{equation}
\end{linenomath*}

such that winter and summer water temperatures are elevated, while spring and fall temperatures are depressed. The season effect is tied to the $\tau_{w}$ value in eq. $\ref{eq4}$ and shifted by a constant ($c$ =.5 or 1.5) to align the coldest day of the year with a seasonal shift of zero. I also include a seasonal expansion factor ($\phi$) that describes the strength of the seasonal effect where zero suggests no effect of season. I limited this effect to be no greater than 3 degrees so as to limit its flexibility and avoid 'reaching' by the model to capture seasonally correlated air temperature readings.

*Temperature HMM*

By evaluating the likelihood of the data ($y_{t}$) given the state models (eq.s $\ref{eq4}$ \& $\ref{eq7}$), weighted by the previous time step's state probabilities (eq. $\ref{eq1}$), it is now possible to make inference about which state ($k=a,w$) the data are in at each time point. Iteratively evaluating eq. $\ref{eq3}$ returns state probabilities that indicate whether the temperature data represent an air or water source as well as the models certainty of that estimate. I implimented this process in the probabilistic programming language Stan [@Stan:2017] with one modification. Given water temperature is the desired state, I expect most datasets will have limited air temperature records likely leading to the inaccurate coincidence of state parameter estimates during the fitting process. Furthermore, tools are currently available to approximate $\alpha_{a}$ and $A_{a}$ with relatively high accuracy over space and time [e.g., @Wang:2012]. Therefore, to avoid coincidence in state parameter estimates which would result in unclear state probabilities, I provide the air state model with known and modelled $\alpha$ and $A$ estimates. Priors indicating estimated parameter value expectations were also incorporated into the model thereby providing coefficient guidance to states with limited data at a given site (see *Model Priors and Known Quantities* for values).

*Observed Data*

To demonstrate a real-world application, I applied our HMM to raw stream temperature data collected at 10 sites in the Thompson River basin in central British Columbia Canada (Fig.$\ref{fig:1}$). Collected for the purpose of understanding geomorphic impacts on stream temperature, these data are not paired with air temperature data and represent a real-world example of the challenges surrounding stream temperature data cleaning. These stream temperature data were collected between July of 2014 and August of 2017 and likely contain examples of all aforementioned error types. Although unconfirmed, I know errors are likely due to evidence upon retrieval but the exact location of those errors in the time series is imprecisely unknown. To compare HMM and traditional methods of data cleaning I manually inspected and labelled data as either air or water using prior knowledge and personal expertise [e.g., @Sowder:2012]. We acknowledge that the true state of temperature is unknown. Eighteen weather stations owned and operated by Environment Canada (Fig.$\ref{fig:1}$), collecting air temperatures during the same period, were consolidated to represent eleven regions in the Thompson River basin and used as a visual reference to compare and contrast modeled source estimates with direct observations.

\begin{figure}[h]
\centering
\includegraphics[width=3.25in]{../images/Map.png}
	\caption{Observed water and air temperarature locations within the Thompson River watershed, nested within the Fraser River watershed (dark grey) in central British Columbia Canada (light grey). Water temperature locations (blue) were monitored between July 2014 and August 2017 concurrently with air temperature stations (yellow) maintained by Enironment Canada. Water temperature locations are labeled by their site number indicating the location of model results shown in figure $\ref{fig:3}$.}
\label{fig:1}
\end{figure}

*Model Priors & Known Quantities*

I expect temperature data in a given state to be more likely to remain in that state than transition to another state. Therefore, I applied weakly informative beta priors with shape parameters $alpha$ = 10 and $beta$ = 2 (median=0.85) for the probability of remaining in the current state and the inverse for the probability of transitioning between states. Moderately informative priors and fixed values were applied to some parameters where more is known about their values. By extracting mean annual air temperatures and associated annual temperature range estimates from the climate tool ClimateBC [@Wang:2012], I was able to provide locally-adjusted (e.g., elevation, latitude) $\alpha_{a}$ and $A_{a}$ parameter estimates. I used ClimateBC air parameter estimates to provide the mean water temperature ($\alpha_{w}$) and amplitude ($A_{w}$) parameters moderately strong priors ($\sigma$ = 5). As the coldest temperatures are widely understood to occur near the first of the year in the northern hemisphere, I estimated $\tau$ to coincide with a cosine curve adjustment of $\pi$. 

I was able to directly calculate the parameter values in eq. $\ref{eq5}$ because temperature cycles occur at regular intervals. Each year typically contains 365 days with 366 in leap years. Thus $\gamma$ is typically some multiplication of 365 depending on the temperature sampling frequency. In this study I collected data every other hour resulting in 4380 observations per cycle. Each observations' $d$ value was calculated as the hour of the year starting with January-01 00:00:00 (M-D HH:MM:SS).

*Model Post-Hoc Calculations*

Manual cleaning of the data suggest air temperatures constitute approximately 4.3\% of the 121,764 data points used in this analysis. This imbalance of data increases the probability of type II errors where water temperature data is inacurrately labeled as air, a problem that is amplified by the HMMs temporal structure. While using previous time-steps' probabilities to inform subsequent probabilities the model takes advantage of temporal auto-correlation and at times overcomes model imprecision, it also increases the potential for perpetuating a state belief beyond local evidence of a state change. This bias can be partially overcome by smoothing the data using the forward-backward algorithm described in @Damiano:2017, which considers the state probabilities in the future as well as the past. Futhermore, rather than use a state certainty break-point probability of 50\% it's possible to adjust this value to reflect the inherent probabilities of each state in the data. Because water data is expected much more frequently than air it makes sense to be more strict when labeling data air and inversely-so when labeling data water. Here I tested disagreements with manual efforts at varied break-points to identify values that limit disagreement and approach manual efforts of cleaning. 

At the same time type I errors are most likely to occur when air data overlap with the water state during daily temperature ocillations. Assuming strong evidence of air temperatures in a day confers the air state to the entire day, it's possible to improve on smoothed estimates and avoid some of these type I errors by considering the totality of the daily data probabilities. Here I took the exponentiated log mean of the water state probabilities for each day, thereby down weighting high likliehood water state values, resulting in singular air values increasing the likelihood of labeling the entire day in the air state. In considering singular and daily level probabilites, we can overcome some of the common transition errors that occur when data states overlap.

#Results

Of the 121,764 observations, the HMM identified 95,514 that had greater than 50\% probability of being water leaving 26,250 observations labeled air. This assignment agreed with the manuial cleaning process 80\% of the time but ranged wildly by site and year. Among sites, disagreement ranged between 6 and 39\% while at the yearly level the range widened to between 1 (site 39 in 2014) and 61\% (site 93 in 2016). Divergence between manual and automated methods was not greater in data limited years such as 2014 and 2017 where only part of the year was recorded nor in site 93 where data was lost after deployment (Fig. $\ref{fig:3}$). 

By relaxing the certainty required to label data in the water state and inversly tightening the requirement for the air state, we reduced the overall disagreement between manual and HMM methods to approximately 7\% (Fig. $\ref{fig:2}$). In so doing we dramatically reduced type II disagreements, while only marginally increasing type I disagreements. The break-point required to reach disagreement minimization is approaching certitude in the air state and nearing complete relaxation of certainty in the water state.

\begin{figure}[h]
\centering
\includegraphics[width=4in]{../images/Error_Type_Obs.png}
	\caption{Shifting disagreement between manual and automated cleaning of stream temperature data as the water certainty break-point decreases. The 'type' of disagreement assumes manual cleaning to be the 'true' state, where a type I error indicates when the HMM labels data water while manual methods indicates air to be the 'true' source. Type II 'errors' occur when the HMM labels data air while the data were manually labeled water. The total percentage of data in disagreement between these data cleaning methods is indicated by the long-dashed line.}
\label{fig:2}
\end{figure}

Using separate break-points of 1\% for individual observations and $1\times10^{-7}$\% for daily probability averages, we do not minimize disagreements between manual and HMM methods but rather operate in a middle ground that does not eliminate water data unnecessarily while identifying the most extreme air temperature records. Using human judgment to identify these break-points we reduced HMM disagreement among sites to as low as 2\% and importantly, only as high as 27\% (Fig. $\ref{fig:3}$). At the annual level this resulted in one site having less than 1\% disagreement (site 49 in 2017) and only one year above 50\% (site 93 in 2016). General agreement between manual and HMM methods increased 4\% to 84\% under these conditions, compared with the default 50\% break-point.

\begin{landscape}

\begin{figure}[h]
\centering
\includegraphics[width=7.5in]{../images/Probs_Obs_Full.png}
	\caption{($\textbf{Left}$) Source estimates for temperature observations at 10 locations within the Thompson River watershed in central British Columbia, Canada . Air (yellow) and water (blue) source estimates were calculated using break-points of greater than 1\% probability for water at the observation level and greater than $1\times10^{-7}$\% when averaged to the daily level. HMM state estimate probabilites are described by the color bar above each time series. State model 95\% confidence intervals describe the probability density window for air (green) and water (purple). Panels are labeled by site number, associated with locations in Fig. $\ref{fig:1}$ and total disagreement (\%). For visual consideration, mean daily air temperature observations at the nearest monitoring station are displayed in red. ($\textbf{Right}$) Temperature observations for site 49 in 2015 and associated source probability estimates. A rug plot in the lower pannel identifies where manual and HMM models disagree and solid lines describe the mean esimate of the annual air and water profiles.}
\label{fig:3}
\end{figure}

\end{landscape}

#Discussion

Here I demonstrate the potential for unsupervised machine learning to clean remotely-sensed data. This approach arguably reduces potential subjectivity and time. Using a well defined HMM the model probabilistically separates water and air temperature signals within a time series using raw temperature observations and a limited number of easily-obtained annual air temperature statistics (Fig. $\ref{fig:3}$). The HMM agreed with manual-cleaning protocols in some sites and years (e.g., sites:1,39,49,80,87), suggesting that with some additional improvements it could become a reliable tool requiring minimal user intervention. The flexibility of the model to accommodate variation among datasets while remaining constrained enough to limit mistakes offers a fair and consistent method for identifying errors. As such, the model may overcome the human limitation of inconsistently applying pattern recognition and offers a greater degree of objectivity. Improving this model to be consistently indistinguishable from high quality manual efforts not only offers freedom from hours of monotony but also promises reduced variation in data quality.

Despite the power and objectivity of this model, there are limitations to this approach and I do not believe humans should be altogether eliminated from the data cleaning process. As demonstrated, 'error' rates change with the certainty threshold (Fig. $\ref{fig:2}$). This suggests a mechanism by which researchers and data managers can contribute to the data cleaning processes in a meaningful but less work intensive way. By adjusting the water's certainty requirements below 50\%, the frequency of type II 'errors' rapidly decrease with only small gains in type I 'errors', resulting in an overall reduction in discrepancies between HMM and manual cleaning (Fig. $\ref{fig:2}$). The cost of incidentally eliminating water data versus retaining air data is largely subjective and dependent on the needs of the research. Often temperature data are summarized such that incidental air data are largely obfuscated in subsequent analysis while the gains of having more complete data that fully describe a period of time lead to larger *n* and greater power. Moreover, the impact of type I 'errors' depends in part on whether or not the data happen to be seasonally coincident. If the expected water temperature is nearly identical to air temperature, the ultimate source may be irrelevant. By building our HMM in a Bayesian framework, uncertainty is fully acknowledged, offering a tool for researchers to assess their studies needs and select the data with sufficient accuracy.

The efficacy of stream temperature state estimation is dependent on the degree of coincidence with the approximated air state, degree of seasonal change in water temperature variation and the degree of hysteresis in the annual temperature cycle. Notably, the HMM struggled to properly estimate a convincing water model when air and water temperatures were strongly coincident. Sites 54, 63, 77, 93 and 100 were particularly afflicted, consistently under-estimating late spring and early summer stream temperatures, resulting in poor agreement with manual data cleaning (Fig. $\ref{fig:3})$. An exploration of these sites revealed they are directly downstream of lakes and are therefore likely reflecting lake temperature dynamics (Fig. $\ref{fig:1}$). The influence of lakes is evident by sudden drops in temperature, indicative of lake turning events where cool water from the lake bottom flushes into the stream due to wind disturbance on the surface [@Wetzel:2001, @Lisi:2015b]. When the lake is thermally stratified, the stream is fed by lake surface waters which strongly reflect low variance air temperatures during the snow free period of the year. As such, the primary distinguishing component between air and water temperature is their variance. Future work could include additional model complexity to capture such dynamics.

The variance between states becomes the only distinguishing characteristic when air and water temperatures overlap, suggesting improved state variance estimates could increase model accuracy. Because many sites have little or no temperature data to inform the air state error term (e.g., site 54), the variance is estimated to be only marginally larger than the water state and only because the variance is ordered (see eq. $\ref{eq6}$). By applying a global model to the air state error term, the HMM may improve its ability to distinguish between states, leveraging estimates at sites where air data are more prevalent. This could also be achieved at the site level by applying a strong and well defined prior on the variance term. 

Hysteresis in the annual temperature cycle is difficult to fit with deterministic trigonometric functions but their rigidity is also necessary to limit model flexibility when HMM states are strongly correlated and coincident. Here seasonal and annual cosine curves capture much of the site-specific hysteresis in water temperatures due to seasonal shifts in weather patterns but do not capture correlations at finer time scales. For instance, site 49 in figure $\ref{fig:3}$ demonstrates a lag in spring temperatures consistent with snow melt cooling the stream despite increased solar radiation [@Lisi:2015]. Once the spring freshet concludes in late June and early July the stream rapidly warms to more closely follow air temperatures. This short period of momentum can not be captured by the rigid underlying water model and as such extends beyond the model's likelihood resulting in data being incorrectly labeled. Typically, short periods of momentum in time series models would be captured in either an auto-regressive (AR) or moving-average (MA) component where a proportion of the observation or error in the previous time step(s) would be included in the subsequent mean estimate [@Shumway:2000]. We were unable to introduce an AR or MA component to our HMM as it made the state models excessively flexible and unable to detect state changes. A possible solution is to allow the water state's error term to fluctuate seasonally (e.g., $\ref{eq7}$) and be described by a distribution that gives higher probabilities to more extreme values (e.g., *t distribution*). This would better accommodate momentum driven auto-correlation, improve water state mean estimates and reduce the likelihood of mislabeling water data (i.e., type II errors).

Big data promises improved accuracy, precision and understanding with increased data volumes. The true power though is through the 'wisdom of the crowds' [@Surowiecki:2004] or drawing upon information across sites, years and days to inform parameter estimates when data is limited. As discussed earlier with variance estimates in the data impoverished air state models, drawing on sites that are data rich to inform data poor sites could extend the model to smaller datasets capturing little of the annual cycle and improve state estimation accuracy. For instance, the parameter $\tau$ adjusts the cosine curve such that the lowest point coincides with the coldest part of the year, a value which is shared among sites with minor variation. By assuming $\tau$ estimates are described by some global distribution, it's possible to improve $\tau$ estimates at sites where data are noisy or missing by considering sites with higher certainty. This could improve the off-set estimate in $\tau$ between the air and water state and better align air state models with the unseen air data (e.g., Fig. $\ref{fig:3}$ site 54). We could extend this idea to the water state's annual mean and amplitude estimates or possibly relate these values to their air temperature estimates derived from the ClimateBC tool. A potential short-coming of global models informing parameters with substantial variation, is the propensity for the local estimate to be pulled towards the global mean. If drawn away from the data sufficiently, the state model is no longer coincident and therefore that state is believed less likely by the HMM. Exploring and tuning global models could prove extremely powerful and extend the HMMs ability beyond the conventional analytical scope of traditional manual methods.

As the volumes of digital data increase so do the opportunities to ask novel, broad-scale and impactful questions that shouldn't be encumbered by the labor costs of ensuring data quality. Here I describe the specification of a Hidden Markov Model that separates air from stream temperature data in order to reduce post-processing QAQC and human subjectivity. The model's construction aims to keep inputs no more complicated than what is typically output by stream temperature loggers and gathered by GPS at the time of deployment. Geo-spatial coordinates can then inform climate models such as ClimateBC or PRISM and approximate air state parameters. The simplicity of the model inputs aim to improve the models appeal and facilitate wide usership. Further developments of this model could improve its accuracy such that humans need only adjust the certainty break-point at which we label data. A simple software-tool could then be developed that allows a user to adjust the breakpoint and observe the changing state of the data until local user knowledge and HMM approximation best align. Subsequently, a *post-hoc* analysis could be fit that fills data-gaps and provides uncertainty estimates resulting in complete datasets useful to a wide variety of ecological studies and monitoring efforts. In these ways, data cleaning tools such as this can facilitate our ability to grow databases, leverage big data in ecological research and tackle increasingly global ecological challenges.

#References
