% Cleaning Ecological Data in the Era of Big Data
% ^1,2^Kyle A. Chezik; ^1^Jonathan W. Moore
% ^1^Earth to Ocean Research Group -- Simon Fraser University 8888 University Dr. Burnaby BC, Canada V5A1S6 -- ^2^778.782.9427 -- kchezik@sfu.ca

#Abstract

#Introduction

Ecological data are often difficult and expensive to collect and despite increasing need for broad-scale ecological studies, funding for robust longitudinal and spatially extensive research is limited. In response, researchers are increasingly relying on digital remote sensing technologies to approximate processes that traditionally would have been observed more directly. (example?) Digital tools can dramatically reduce the cost of data collection but are often indirect measures of the biological process in question. (example?) Increased uncertainty do the proximal nature of these data is often compensated for in volume at higher spatial and temporal resolution than could be accomplished by even well funded tradtional approaches. For example, LiDAR is a remote sensing tool used to collect three dimensional images [e.g., @WADNR:LiDAR]. A single flight over a forest can map individual trees and be used to measure forest growth [e.g., @Caughlin:2016], canopy density [@Lee:2007], terrain topology [e.g., @WADNR:LiDAR], photosynthetic rate and even species composition [e.g., @Barbosa:2017; @Asner:2017]. Although a technology like LiDAR is currently expensive, tools such as these are often cost shared across research projects and labs, and in the future production efficiencies will reduce entry barriers. This process has already played out in many other sensor technologies like GPS, oxygen sensors, light sensors and temperature sensors, camera traps. The cost reduction and miniturization of these technologies have made mass implimentation easy and affordable. 

The quantity of data returned by remote sensing devices is both a blessing and a curse. Although, we can now collect large quantities of data over broad space and time horizons which can return refined understandings of ecosystem processes, data errors are also more likely to go unnoticed without rigorous and time consuming analysis.Although it is tempting to assume data volume will obscure the influence of erroneous data, systematic bias can lead to inaccurate conclusions which once uncovered will hurt the credability of the research or even science more generally. So while the struggle of collecting data in the field may be reduced, the effort in post-processing those data has greatly increased. Clever post-processing tools have been developed that use a variety of rule based algorithms to eliminate many known errors. This method of error correction is referred to by Pedro Domingos as knowledge engineering [@Domingos:2015], which is wholey dependant on humans to make sense of new and unusual errors as they arise. Programing logic to identify each new source of erroneous data is laborious and often leads to complexity limitations as the number of influential variables and their combinations compound. 

Increasingly the solution to the problem of insurmountable complexity in big data is machine learning. Neural networks, support vector machines (SVM), principle component analysis (PCA), naive bayes, etc., are common algorithms for reducing the dimensionality of data and grouping by similarity (i.e., shortest distance). In so doing these methods infer underlying processes that group data without being explicity directed by a rule bound algorithm. The limitation here is that often the algorithm needs many labeled data to discern clear boundaries between data types but frequently the true nature of data are unknown rendering many of these methods useless. Even PCA and Isomap, which can group raw data by similarity through dimensionality reduction, will not necessarily identify an erroneous group if these data share dominating properties with error free data. To identify errors in data, which are inherently unlabeled, we need to employ unsupervised learning methods that leverage known physical processes to constrain the learning process but also limits the amount of human supervision so the data themeselves can provide the discerning information as new errors are identified. One such method is Hidden Markov Models (HMM) where potential states are provided and the probability of being in one state is estimated given the data. In this way, HMMs are at the intersection of human and machine inteligence where humans provide conceptual models from an a prior understanding of the processes underlying the data and then the machine bins the data under these constraints but uses data as evidence to optimize the binning proceedure.

Water temperature governs aquatic biological processes and is a fundamental and essential component to understanding aquatic ecology. .... Flowing water is especially thermally dynamic and essential to understanding lotic freshwater ecological systems.

Monitoring stream water temperature has become a relatively trival and routine task due to the development of relatively cheap, long-lived, small and environmentally robust temperature sensors. As a result, sensors are being deployed at increasingly high spatial and temporal resolution, resulting in exponentially growing volumes of data even for the simplest of studies.

#Methods

#Results

#Discussion

#References
